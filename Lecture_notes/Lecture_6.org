#+title:  Linear regression
#+author: Daniel Ballesteros-Chávez
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.1 (Org mode 9.3.6)
#+PROPERTY: header-args :R+ :exports both
#+PROPERTY: header-args :R+ :session *R*
#+HTML_HEAD: <style type="text/css"> tr:nth-child(odd) {background-color: #e2e2e2;}  tr:first-child {font-weight: bold}  tr:hover {background-color: #d0c6e5;}</style>
#+HTML_HEAD_EXTRA: <style>code {background-color: #ccc}</style>
:results:
#+HTML_HEAD:<style>
#+HTML_HEAD:/* Daniel Ballesteros-Chavez */
#+HTML_HEAD:/* DBCh CSS for blog project */
#+HTML_HEAD:/* color schemes: #333745; #E63462 ; #C7EFCF ; #EEF5DB ; #909396; #262626;*/
#+HTML_HEAD:/* Modified version with responsive TOC
#+HTML_HEAD:
#+HTML_HEAD:/* usage: #+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./style01.css"/> */
#+HTML_HEAD:
#+HTML_HEAD:body {
#+HTML_HEAD:	font-size: 18px;
#+HTML_HEAD:	color: #404040;
#+HTML_HEAD:	/* background-color: #333745; */
#+HTML_HEAD:	font-family: Helvetica;
#+HTML_HEAD:	line-height: 1.3;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#content {
#+HTML_HEAD:	max-width: 50em;
#+HTML_HEAD:	margin-left: auto;
#+HTML_HEAD:	margin-right: auto;
#+HTML_HEAD:    padding: 15px 50px 50px 15px;
#+HTML_HEAD:    background-color: #E4F7FF;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:p {
#+HTML_HEAD:		text-align: justify;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:/* this part is about the table of contents TOC */
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents a:link,
#+HTML_HEAD:#table-of-contents a:visited {
#+HTML_HEAD:    color: #404040;
#+HTML_HEAD:    background: transparent;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents a:hover {
#+HTML_HEAD:  background-color: #ccc;
#+HTML_HEAD:  color: #404040;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents {
#+HTML_HEAD:    line-height: 1.2;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents h2 {
#+HTML_HEAD:    background-color:  #ccc ;
#+HTML_HEAD:    padding-left: 0.3em;
#+HTML_HEAD:    color: #404040;
#+HTML_HEAD:    border-bottom: 0;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents ul {
#+HTML_HEAD:    list-style: none;
#+HTML_HEAD:    padding-left: 0.3em;
#+HTML_HEAD:    font-weight: normal;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents div>ul>li {
#+HTML_HEAD:    margin-top: 1em;
#+HTML_HEAD:    font-weight: bold;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents .tag {
#+HTML_HEAD:    display: none;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents .todo,
#+HTML_HEAD:#table-of-contents .done {
#+HTML_HEAD:    font-size: 80%;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents ol>li {
#+HTML_HEAD:    margin-top: 1em;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:@media screen {
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents {
#+HTML_HEAD:        position: fixed;
#+HTML_HEAD:        top: 0;
#+HTML_HEAD:        left: 0;
#+HTML_HEAD:        padding: 1em 0 1em 1em;
#+HTML_HEAD:        width: 290px;
#+HTML_HEAD:        height: 100vh;
#+HTML_HEAD:        overlow-x: hidden;
#+HTML_HEAD:        overlow-y: auto;
#+HTML_HEAD:	overflow: auto;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents h2 {
#+HTML_HEAD:        margin-top: 0;
#+HTML_HEAD:        font-family: Helvetica,Arial,"Lucida Grande",sans-serif;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents code {
#+HTML_HEAD:        font-size: 12px;
#+HTML_HEAD:    }
#+HTML_HEAD:    
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:@media screen and (max-width: 95em) {
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents {
#+HTML_HEAD:        display: none;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    h1.title {
#+HTML_HEAD:        margin-left: 0%;
#+HTML_HEAD:    }
#+HTML_HEAD:    
#+HTML_HEAD:    div#content {
#+HTML_HEAD:        margin-left: 5%;
#+HTML_HEAD:        max-width: 90%;
#+HTML_HEAD:    }
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:/*Html Boxes around THMs and Propositions */
#+HTML_HEAD:.abstract  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:	border: 1px solid #404040;
#+HTML_HEAD:    box-shadow: 3px 3px 3px ;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .abstract:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Abstract';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.mydef  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid #404040;
#+HTML_HEAD:    background-color: #FFD580;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px orange; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .mydef:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    /* background-color: white; */
#+HTML_HEAD:    background-color: orange;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Definition';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.prop  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid ;
#+HTML_HEAD:    background-color: #F1FFC2;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px green; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .prop:before {
#+HTML_HEAD:    	color:  white;
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: green;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Proposition';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.thm  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid ;
#+HTML_HEAD:    background-color: lightcyan;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px brown; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .thm:before {
#+HTML_HEAD:    	color:  white;
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: darkblue;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Theorem';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:  .cor  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid blue;
#+HTML_HEAD:    box-shadow: 3px 3px 3px blue;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .cor:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Corollary';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:/*defaults form org-mode export */
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:  .title  { text-align: center; }
#+HTML_HEAD:  .todo   { font-family: monospace; color: red; }
#+HTML_HEAD:  .done   { color: green; }
#+HTML_HEAD:  .tag    { background-color: #eee; font-family: monospace;
#+HTML_HEAD:            padding: 2px; font-size: 80%; font-weight: normal; }
#+HTML_HEAD:  .timestamp { color: #bebebe; }
#+HTML_HEAD:  .timestamp-kwd { color: #5f9ea0; }
#+HTML_HEAD:  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
#+HTML_HEAD:  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
#+HTML_HEAD:  .center { margin-left: auto; margin-right: auto; text-align: center; }
#+HTML_HEAD:  .underline { text-decoration: underline; }
#+HTML_HEAD:  #postamble p, #preamble p { font-size: 90%; margin: .2em; text-align: center;}
#+HTML_HEAD:  p.verse { margin-left: 3%; }
#+HTML_HEAD:  pre {
#+HTML_HEAD:    border: 1px solid #ccc;
#+HTML_HEAD:    box-shadow: 3px 3px 3px #eee;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    font-family: monospace;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src {
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src:before {
#+HTML_HEAD:    display: none;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -10px;
#+HTML_HEAD:    right: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src:hover:before { display: inline;}
#+HTML_HEAD:  pre.src-sh:before    { content: 'sh'; }
#+HTML_HEAD:  pre.src-bash:before  { content: 'sh'; }
#+HTML_HEAD:  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
#+HTML_HEAD:  pre.src-R:before     { content: 'R'; }
#+HTML_HEAD:  pre.src-perl:before  { content: 'Perl'; }
#+HTML_HEAD:  pre.src-java:before  { content: 'Java'; }
#+HTML_HEAD:  pre.src-sql:before   { content: 'SQL'; }
#+HTML_HEAD:
#+HTML_HEAD:  table { border-collapse:collapse; }
#+HTML_HEAD:  caption.t-above { caption-side: top; }
#+HTML_HEAD:  caption.t-bottom { caption-side: bottom; }
#+HTML_HEAD:  td, th { vertical-align:top;  }
#+HTML_HEAD:  th.right  { text-align: center;  }
#+HTML_HEAD:  th.left   { text-align: center;   }
#+HTML_HEAD:  th.center { text-align: center; }
#+HTML_HEAD:  td.right  { text-align: right;  }
#+HTML_HEAD:  td.left   { text-align: left;   }
#+HTML_HEAD:  td.center { text-align: center; }
#+HTML_HEAD:  dt { font-weight: bold; }
#+HTML_HEAD:  .footpara:nth-child(2) { display: inline; }
#+HTML_HEAD:  .footpara { display: block; }
#+HTML_HEAD:  .footdef  { margin-bottom: 1em; }
#+HTML_HEAD:  .figure { padding: 1em; }
#+HTML_HEAD:  .figure p { text-align: center; }
#+HTML_HEAD:  .inlinetask {
#+HTML_HEAD:    padding: 10px;
#+HTML_HEAD:    border: 2px solid gray;
#+HTML_HEAD:    margin: 10px;
#+HTML_HEAD:    background: #ffffcc;
#+HTML_HEAD:  }
#+HTML_HEAD:  #org-div-home-and-up
#+HTML_HEAD:   { text-align: right; font-size: 70%; white-space: nowrap; }
#+HTML_HEAD:  textarea { overflow-x: auto; }
#+HTML_HEAD:  .linenr { font-size: smaller }
#+HTML_HEAD:  .code-highlighted { background-color: #ffff00; }
#+HTML_HEAD:  .org-info-js_info-navigation { border-style: none; }
#+HTML_HEAD:  #org-info-js_console-label
#+HTML_HEAD:    { font-size: 10px; font-weight: bold; white-space: nowrap; }
#+HTML_HEAD:  .org-info-js_search-highlight
#+HTML_HEAD:    { background-color: #ffff00; color: #000000; font-weight: bold; }
#+HTML_HEAD:
#+HTML_HEAD:</style>
:end:


* Introduciton

+ The idea behind the linear regression model is that we would like to
  predict a dependent variable $Y$ using the independent variable $X$
  assuming that they are related in a linear way.


+ This is a very simple assumption, nevertheless, linear regression is
  still a useful and widely used statistical method.

+ Many fancy statistical learning approaches can be seen as
  generalizations or extensions of linear regression. Consequently,
  the importance of having a good understanding of linear regression
  before studying more complex learning methods cannot be overstated.



This is a very basic
approach for predicting a quantitative response $Y$ on the basis of a sin-
gle predictor variable $X$. It assumes that there is approximately a linear
relationship between $X$ and $Y$.

\begin{equation}
Y  \approx  \alpha + \beta X
\end{equation}


Where the constants $\alpha$ and $\beta$ will be determined.
+ $\alpha$ is called the intercept.
+ $\beta$ is called the slope.


Hence, we would like to estimate the values of $Y$ on the basis of $X = x$. We will denote
this estimated value by $\hat{y}$. Usually the intercept and slope are also unknown, so they are to
be estimated. In this case we write
\begin{equation}
\hat{y} = \hat{\alpha} + \hat{\beta}x.
\end{equation}


* Estimating the Coefficients

In order to estimate the coefficitents $\alpha$ and $\beta$, we will
use the data we have at our disposal, for instance $\{ x_1, x_2,
\ldots, x_n\}$ and $\{ y_1, y_2, \ldots, y_n\}$.


Then we want to find the coefficient esitmates $\hat{\alpha}$ and $\hat{\beta}$ such that for
$i = 1, 2, \ldots, n$ we have
\begin{equation}
y_i \approx \hat{\alpha} + \hat{\beta}x_i.
\end{equation}

We want to find the straight line determined by $\hat{\alpha}$ and $\hat{\beta}$ 
that is as closed as possible to the $n$ pairs of points $(x_i, y_i) \in \mathbb{R}^2$, $i =1, 2, 
\ldots,n$.



For this we need a metric to decide what do we mean by /to be close to the points/.

Define the $i$ -th residual as the difference between the $i$ -th observed response (value of Y) and 
the $i$ -th predicted value by our estimation:
\begin{equation}
\begin{split}
\epsilon_i &= y_i - \hat{y}_i \\
&= y_i - \hat{\alpha} - \hat{\beta}x_i \\
\end{split}
\end{equation}



Now we define the function $g:\mathbb{R}^2 \to \mathbb{R}$ as

\begin{equation}
\begin{split}
g(\alpha, \beta)& = \sum_{i=1}^n \epsilon_i^2\\
&=\sum_{i=1}^{n}\left( y_i - \hat{\alpha} - \hat{\beta}x_i \right)^2.\\
\end{split}
\end{equation}

This function $g$ is usually called  residual sum of squares (RSS).


* Least squares method

By solving $\partial g/\partial \alpha = 0$ and $\partial g/ \partial \beta = 0$ one gets the cirtical point

\begin{equation}
\begin{split}
\hat{\beta} & = \frac{ \sum_{i=1}^{n} \left(x_i - \bar{x}\right)(y_i - \bar{y}) }{\sum_{i=1}^{n}( x_i - \bar{x})^2},\\
\hat{\alpha}& = \bar{y} - \hat{\beta}\bar{x},
\end{split}
\end{equation}

where

\begin{equation}
\bar{x}  = \frac{1}{n}\sum_{i=1}^{n} x_i, \quad \mbox{and}\quad \bar{y}  = \frac{1}{n}\sum_{i=1}^{n} y_i
\end{equation}

* Covariance

** Definition
   

For two jointly distributed real-valued random variables $X$ and $Y$ ( both with finite variance), the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values:
\begin{equation}
\displaystyle \operatorname {cov} (X,Y)=\operatorname {E} {{\big [}(X-\operatorname {E} [X])(Y-\operatorname {E} [Y]){\big ]}} 
\end{equation}

If the random variables are descrete, this can also be writte as

\begin{equation}
\displaystyle \operatorname {cov} (X,Y)=\frac{1}{n}\sum_{i=1}^{n} {{\big [}(X_i-\overline{X})(Y_{i}-\overline{Y}){\big ]}} 
\end{equation}
 

* Second method to obtain the coefficients

By writing $\overline{X} = E[X]$, and the last definition then it is possible to write

\begin{equation}
\begin{split}
\hat{\beta}  &= \frac{\operatorname{cov} (X,Y)}{\operatorname{var}(X)},\\
\hat{\alpha} &= \operatorname{E}[Y] - \beta\operatorname{E}[X] .
\end{split}
\end{equation}

* Standard errors

Recall that by /standard error/ (deviation), we mean the  square root of the variance of an estimator,
in case of our estimate for the coefficients:

\begin{equation}
\begin{split}
\operatorname{SE}^2(\hat{\alpha}) &= \sigma^2\left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \overline{x})^2}\right], \\
\operatorname{SE}^2(\hat{\beta}) &= \frac{\sigma^2}{\sum_{i=1}^n (x_i - \overline{x})^2}.
\end{split}
\end{equation}

Where  $\sigma^2 = \mbox{Var}(\epsilon)$. In general, $\sigma$ is not known, but can be estimated from the data. 
This estimate is known as the /residual standard error/, and is given by the formula

\begin{equation}
\mbox{RSE} = \sqrt{\frac{\mbox{RSS}}{n-2}}
\end{equation}

Roughly speaking, it is the average amount that the response
will deviate from the true regression line.

* Assessing the Accuracy of the Model

Standard errors can also be used to perform hypothesis tests on the
coefficients. The most common hypothesis test involves testing the null
hypothesis of

$H_0$ : There is no relationship between $X$ and $Y$.

$H_1$ : There is a relationship between $X$ and $Y$.

Equivalently:

\begin{equation}
\begin{split}
H_0 : \beta = 0,\\
H_1 : \beta \neq 0.\\
\end{split}
\end{equation}


This usually uses the $t$ -statistic
\begin{equation}
t = \frac{\hat{\beta} - 0}{\mbox{SE}(\hat{\beta})}.
\end{equation}

If there really is no relationship between X and Y , then we expect
that $t$ will have a t-distribution with n − 2 degrees of freedom.

/We reject the null hypothesis/ — that is, we declare a relationship
to exist between X and Y — if the p-value is small enough.

* Example

The linear model is implemented in R via the function =lm=.

From the data set iris, contrast the columns for /Sepal length/ and 
/Petal length/ and fit a linear model:
** 
# :PROPERTIES:
# :BEAMER_OPT: shrink=50
# :END:

\tiny
#+begin_example R
df  <-  iris
model01  <-  lm(df$Sepal.Length ~ df$Petal.Length)
summary(model01)

Call:
lm(formula = df$Sepal.Length ~ df$Petal.Length)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.24675 -0.29657 -0.01515  0.27676  1.00269 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)      4.30660    0.07839   54.94   <2e-16 ***
df$Petal.Length  0.40892    0.01889   21.65   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4071 on 148 degrees of freedom
Multiple R-squared:   0.76,	Adjusted R-squared:  0.7583 
F-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16

#+end_example

#+RESULTS:

* $R^2$ - statistic

The $R^2$ statistic is a measure of the linear (sample) relationship between $X$ and
$Y$ namely $\mbox{cor}(X,Y)$. It is also a measure of the linear relationship between X and Y .

\begin{equation}
R^2 = \frac{\mbox{TSS} - \mbox{RSS}}{\mbox{TSS}}  = 1 - \frac{\mbox{RSS}}{\mbox{TSS}}
\end{equation}

Here

\begin{equation}
\mbox{TSS} = \sum_{i=1}^{n} (y_i - \overline{y})^2.
\end{equation}


An $R^2$ statistic that is
close to 1 indicates that a large proportion of the variability in the response
has been explained by the regression. A number near 0 indicates that the
regression did not explain much of the variability in the response; this might
occur because the linear model is wrong, or the inherent error $\sigma^2$ is high,
or both.


* $F$ - statistic

This is used in multiple linear regression when we want to fit 
\begin{equation}
Y = \alpha + \sum_{j=1}^{p} \beta_{j} X_j + \epsilon.
\end{equation}

It is defined by
\begin{equation}
F = \frac{\mbox{TSS} - \mbox{RSS}}{\mbox{RSS}} \cdot \frac{ n-p-1}{p}.
\end{equation}

Hence, when there is no relationship between the response and predictors,
one would expect the F-statistic to take on a value close to 1. And on the other hand
if $F$ is greater than $1$ then the hypothesis that at least one of the $\beta_i$'s is
different from zero have strong evidence.


* Multiple linear regressions.

Simple linear regression is a useful approach for predicting a response on the
basis of a single predictor variable. However, in practice we often have more
than one predictor.

One option is to run three separate simple linear regressions, each of
which uses a different independent variable as a predictor.

Instead of fitting a separate simple linear regression model for each predictor,
 a better approach is to extend the simple linear regression model
so that it can directly accommodate multiple predictors.

This is used in multiple linear regression when we want to fit 
\begin{equation}
Y = \alpha + \sum_{j=1}^{p} \beta_{j} X_j + \epsilon.
\end{equation}



* Estimating the Regression Coefficients

As in the case of Simple linear regression, we look to minimise the residual sum squared:

\begin{equation}
\mbox{RSS} = \sum_{i=1}^{n} (y_i - \alpha - \beta_1 x_{i1} - \beta_2 x_{i2} - \cdots \beta_p x_{ip} ) ^2.
\end{equation}


And we proceed as in the  simple linear case.

* Important questions


1) Is at least one of the predictors $X_1 , X_2 , \dots , X_p$ useful in predicting the response?
2)  Do all the predictors help to explain Y , or is only a subset of the predictors useful?
3)  How well does the model fit the data?
4) Given a set of predictor values, what response value should we predict, and how accurate is our prediction?


* Is at least one of the predictors useful?

The approach of using an $F$ -statistic to test for any association between
the predictors and the response works when $p$ is relatively small, and 
certainly small compared to $n$. However, sometimes we have a very large number
 of variables. If p > n then there are more coefficients $\beta_j$ to estimate
than observations from which to estimate them. In this case we cannot
even fit the multiple linear regression model using least squares, so the
$F$ -statistic cannot be used, and neither can most of the other concepts that
we have seen so far


* Deciding on Important Variables

Ideally, we would like to perform variable selection by trying out a lot of
different models, each containing a different subset of the predictors. For
instance, if $p = 2$, then we can consider four models: 

1) a model containing no variables, 
2) a model containing $X_1$ only, 
3) a model containing $X_2$ only, 
4) a model containing both $X_1$ and $X_2$ .


* Deciding on Important Variables

We can then select the best model out of all of the models that we have considered. How
do we determine which model is best? Various statistics can be used to
judge the quality of a model. 

These include 

+ Mallow’s C_p , 
+ Akaike informa-tion criterion (AIC), 
+ Bayesian information criterion (BIC), 
+ Adjusted $R^2$.


* Deciding on Important Variables

Unless p is very
small, we cannot consider all $2^p$ models, and instead we need an automated
and efficient approach to choose a smaller set of models to consider. There
are three classical approaches for this task:

** Forward selection

We begin with the null model—a model that con-
tains an intercept but no predictors. We then fit p simple linear re-
gressions and add to the null model the variable that results in the
lowest RSS. We then add to that model the variable that results
in the lowest RSS for the new two-variable model. This approach is
continued until some stopping rule is satisfied.

* 

** Backward selection.

We start with all variables in the model, and
remove the variable with the largest p-value—that is, the variable
that is the least statistically significant. The new $(p - 1)$ -variable
model is fit, and the variable with the largest p-value is removed. This
procedure continues until a stopping rule is reached. For instance, we
may stop when all remaining variables have a p-value below some
threshold.
Backward selection cannot be used if $p > n$.

** Mixed selection

This is a combination of forward and backward se-
lection. We start with no variables in the model, and as with forward
selection, we add the variable that provides the best fit. We con-
tinue to add variables one-by-one.

If at any point the
p-value for one of the variables in the model rises above a certain
threshold, then we remove that variable from the model. We con-
tinue to perform these forward and backward steps until all variables
in the model have a sufficiently low p-value, and all variables outside
the model would have a large p-value if added to the model.


* Model Fit

Two of the most common numerical measures of model fit are the RSE and
$R^2$ , the fraction of variance explained. These quantities are computed and
interpreted in the same fashion as for simple linear regression.

* Predictions

There are three sorts of uncertainty associated with the Linear Model 

+ We can compute a confidence interval in order to determine how close $\hat{Y}$ will be to $f(X)$.
+ Model bias: we use a linear model, we are in fact estimating the best linear approximation to the true surface. However, here we will ignore this discrepancy, and operate as if the linear model were correct.
+ We use prediction intervals to answer how much will $Y$ vary from
  $\hat{Y}$ ponitwise. Prediction intervals are always wider than
  confidence intervals, because they incorporate both the error in the
  estimate for f (X) (the reducible error) and the uncertainty as to
  how much an individual point will differ from the population
  regression plane


I found this nice video.

[[https://www.youtube.com/watch?v=qVCQi0KPR0s]]


* Example
:PROPERTIES:
:BEAMER_OPT: shrink=40
:END:



#+begin_example
fit <- lm(Petal.Width ~ Petal.Length, data=iris)

plot(Petal.Width ~ Petal.Length, col=c("black", "red", "blue")[Species], 
pch=(15:17)[Species], 
xlab="Petal Length (cm)", ylab="Petal Width (cm)", data=iris)
newx <- data.frame(Petal.Length=seq(min(iris$Petal.Length), max(iris$Petal.Length), length.out=100))
conf.interval <- predict(fit, newdata=newx, interval="confidence")
pred.interval <- predict(fit, newdata=newx, interval="prediction")
lines(conf.interval[, "fit"] ~ newx[, 1], lty=1, lw=3)
lines(conf.interval[, "lwr"] ~ newx[, 1], lty=2)
lines(conf.interval[, "upr"] ~ newx[, 1], lty=2)
lines(pred.interval[, "lwr"] ~ newx[, 1], lty=3)
lines(pred.interval[, "upr"] ~ newx[, 1], lty=3)
legend("topleft", legend=c(levels(iris$Species), "CI", "PI"), 
col=c("black", "red", "blue", "black", "black"), 
pch=c(15:17, -1, -1), lty=c(-1, -1, -1, 2, 3))
#+end_example


from [[https://rpubs.com/lwaldron/iris_regression]]

* References

[1] Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani /An Introduction to Statistical Learning/. Springer Texts in Statistics, 2014.
