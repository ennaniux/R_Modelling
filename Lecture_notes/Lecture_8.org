#+title: The logistic regression
#+author: Daniel Ballesteros-Ch√°vez
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.1 (Org mode 9.3.6)
#+PROPERTY: header-args :R+ :exports both
#+PROPERTY: header-args :R+ :session *R*
#+HTML_HEAD: <style type="text/css"> tr:nth-child(odd) {background-color: #e2e2e2;}  tr:first-child {font-weight: bold}  tr:hover {background-color: #d0c6e5;}</style>
#+HTML_HEAD_EXTRA: <style>code {background-color: #ccc}</style>
:results:
#+HTML_HEAD:<style>
#+HTML_HEAD:/* Daniel Ballesteros-Chavez */
#+HTML_HEAD:/* DBCh CSS for blog project */
#+HTML_HEAD:/* color schemes: #333745; #E63462 ; #C7EFCF ; #EEF5DB ; #909396; #262626;*/
#+HTML_HEAD:/* Modified version with responsive TOC
#+HTML_HEAD:
#+HTML_HEAD:/* usage: #+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./style01.css"/> */
#+HTML_HEAD:
#+HTML_HEAD:body {
#+HTML_HEAD:	font-size: 18px;
#+HTML_HEAD:	color: #404040;
#+HTML_HEAD:	/* background-color: #333745; */
#+HTML_HEAD:	font-family: Helvetica;
#+HTML_HEAD:	line-height: 1.3;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#content {
#+HTML_HEAD:	max-width: 50em;
#+HTML_HEAD:	margin-left: auto;
#+HTML_HEAD:	margin-right: auto;
#+HTML_HEAD:    padding: 15px 50px 50px 15px;
#+HTML_HEAD:    background-color: #E4F7FF;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:p {
#+HTML_HEAD:		text-align: justify;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:/* this part is about the table of contents TOC */
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents a:link,
#+HTML_HEAD:#table-of-contents a:visited {
#+HTML_HEAD:    color: #404040;
#+HTML_HEAD:    background: transparent;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents a:hover {
#+HTML_HEAD:  background-color: #ccc;
#+HTML_HEAD:  color: #404040;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents {
#+HTML_HEAD:    line-height: 1.2;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents h2 {
#+HTML_HEAD:    background-color:  #ccc ;
#+HTML_HEAD:    padding-left: 0.3em;
#+HTML_HEAD:    color: #404040;
#+HTML_HEAD:    border-bottom: 0;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents ul {
#+HTML_HEAD:    list-style: none;
#+HTML_HEAD:    padding-left: 0.3em;
#+HTML_HEAD:    font-weight: normal;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents div>ul>li {
#+HTML_HEAD:    margin-top: 1em;
#+HTML_HEAD:    font-weight: bold;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents .tag {
#+HTML_HEAD:    display: none;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents .todo,
#+HTML_HEAD:#table-of-contents .done {
#+HTML_HEAD:    font-size: 80%;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents ol>li {
#+HTML_HEAD:    margin-top: 1em;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:@media screen {
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents {
#+HTML_HEAD:        position: fixed;
#+HTML_HEAD:        top: 0;
#+HTML_HEAD:        left: 0;
#+HTML_HEAD:        padding: 1em 0 1em 1em;
#+HTML_HEAD:        width: 290px;
#+HTML_HEAD:        height: 100vh;
#+HTML_HEAD:        overlow-x: hidden;
#+HTML_HEAD:        overlow-y: auto;
#+HTML_HEAD:	overflow: auto;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents h2 {
#+HTML_HEAD:        margin-top: 0;
#+HTML_HEAD:        font-family: Helvetica,Arial,"Lucida Grande",sans-serif;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents code {
#+HTML_HEAD:        font-size: 12px;
#+HTML_HEAD:    }
#+HTML_HEAD:    
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:@media screen and (max-width: 95em) {
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents {
#+HTML_HEAD:        display: none;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    h1.title {
#+HTML_HEAD:        margin-left: 0%;
#+HTML_HEAD:    }
#+HTML_HEAD:    
#+HTML_HEAD:    div#content {
#+HTML_HEAD:        margin-left: 5%;
#+HTML_HEAD:        max-width: 90%;
#+HTML_HEAD:    }
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:/*Html Boxes around THMs and Propositions */
#+HTML_HEAD:.abstract  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:	border: 1px solid #404040;
#+HTML_HEAD:    box-shadow: 3px 3px 3px ;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .abstract:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Abstract';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.mydef  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid #404040;
#+HTML_HEAD:    background-color: #FFD580;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px orange; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .mydef:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    /* background-color: white; */
#+HTML_HEAD:    background-color: orange;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Definition';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.prop  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid ;
#+HTML_HEAD:    background-color: #F1FFC2;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px green; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .prop:before {
#+HTML_HEAD:    	color:  white;
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: green;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Proposition';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.thm  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid ;
#+HTML_HEAD:    background-color: lightcyan;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px brown; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .thm:before {
#+HTML_HEAD:    	color:  white;
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: darkblue;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Theorem';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:  .cor  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid blue;
#+HTML_HEAD:    box-shadow: 3px 3px 3px blue;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .cor:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Corollary';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:/*defaults form org-mode export */
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:  .title  { text-align: center; }
#+HTML_HEAD:  .todo   { font-family: monospace; color: red; }
#+HTML_HEAD:  .done   { color: green; }
#+HTML_HEAD:  .tag    { background-color: #eee; font-family: monospace;
#+HTML_HEAD:            padding: 2px; font-size: 80%; font-weight: normal; }
#+HTML_HEAD:  .timestamp { color: #bebebe; }
#+HTML_HEAD:  .timestamp-kwd { color: #5f9ea0; }
#+HTML_HEAD:  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
#+HTML_HEAD:  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
#+HTML_HEAD:  .center { margin-left: auto; margin-right: auto; text-align: center; }
#+HTML_HEAD:  .underline { text-decoration: underline; }
#+HTML_HEAD:  #postamble p, #preamble p { font-size: 90%; margin: .2em; text-align: center;}
#+HTML_HEAD:  p.verse { margin-left: 3%; }
#+HTML_HEAD:  pre {
#+HTML_HEAD:    border: 1px solid #ccc;
#+HTML_HEAD:    box-shadow: 3px 3px 3px #eee;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    font-family: monospace;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src {
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src:before {
#+HTML_HEAD:    display: none;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -10px;
#+HTML_HEAD:    right: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src:hover:before { display: inline;}
#+HTML_HEAD:  pre.src-sh:before    { content: 'sh'; }
#+HTML_HEAD:  pre.src-bash:before  { content: 'sh'; }
#+HTML_HEAD:  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
#+HTML_HEAD:  pre.src-R:before     { content: 'R'; }
#+HTML_HEAD:  pre.src-perl:before  { content: 'Perl'; }
#+HTML_HEAD:  pre.src-java:before  { content: 'Java'; }
#+HTML_HEAD:  pre.src-sql:before   { content: 'SQL'; }
#+HTML_HEAD:
#+HTML_HEAD:  table { border-collapse:collapse; }
#+HTML_HEAD:  caption.t-above { caption-side: top; }
#+HTML_HEAD:  caption.t-bottom { caption-side: bottom; }
#+HTML_HEAD:  td, th { vertical-align:top;  }
#+HTML_HEAD:  th.right  { text-align: center;  }
#+HTML_HEAD:  th.left   { text-align: center;   }
#+HTML_HEAD:  th.center { text-align: center; }
#+HTML_HEAD:  td.right  { text-align: right;  }
#+HTML_HEAD:  td.left   { text-align: left;   }
#+HTML_HEAD:  td.center { text-align: center; }
#+HTML_HEAD:  dt { font-weight: bold; }
#+HTML_HEAD:  .footpara:nth-child(2) { display: inline; }
#+HTML_HEAD:  .footpara { display: block; }
#+HTML_HEAD:  .footdef  { margin-bottom: 1em; }
#+HTML_HEAD:  .figure { padding: 1em; }
#+HTML_HEAD:  .figure p { text-align: center; }
#+HTML_HEAD:  .inlinetask {
#+HTML_HEAD:    padding: 10px;
#+HTML_HEAD:    border: 2px solid gray;
#+HTML_HEAD:    margin: 10px;
#+HTML_HEAD:    background: #ffffcc;
#+HTML_HEAD:  }
#+HTML_HEAD:  #org-div-home-and-up
#+HTML_HEAD:   { text-align: right; font-size: 70%; white-space: nowrap; }
#+HTML_HEAD:  textarea { overflow-x: auto; }
#+HTML_HEAD:  .linenr { font-size: smaller }
#+HTML_HEAD:  .code-highlighted { background-color: #ffff00; }
#+HTML_HEAD:  .org-info-js_info-navigation { border-style: none; }
#+HTML_HEAD:  #org-info-js_console-label
#+HTML_HEAD:    { font-size: 10px; font-weight: bold; white-space: nowrap; }
#+HTML_HEAD:  .org-info-js_search-highlight
#+HTML_HEAD:    { background-color: #ffff00; color: #000000; font-weight: bold; }
#+HTML_HEAD:
#+HTML_HEAD:</style>
:end:



* Introduction

The linear regression model discussed in previous lectures assumes that the response variable $Y$ is quantitative. But in many situations, the response
variable is instead qualitative.

Often qualitative variables are referred to as /categorical/.

Then, the problem of predicting in this case is a problem of *classification*.

* Classification and the difficulties with linear regression approach

Some difficulties arise when we try to use a simple linear regression model in 
order to classify some data into some categories.

In the best scenario, the response variable‚Äôs values take on a natural ordering, such as
mild, moderate, and severe, and hopefully we also have that the gap between mild and moderate
is similar to the gap between moderate and severe, then a 1, 2, 3 coding
would be reasonable. 

Unfortunately, in general there is no natural way to
convert a qualitative response variable with more than two levels into a
quantitative response that is ready for linear regression.


For a binary (two level) qualitative response, the situation is better. It does not matter
if we flip the coding we use for the values of the $Y$ -variable, the linear regression will
produce the same predictors.


* Binary case

For a binary response with a 0/1 coding as above, regression by least
squares does make sense; it can be shown that the estimate $\hat{\beta}X$ obtained using linear
regression is in fact an estimate of $Pr( Y=y |X)$ in this special
case. However, if we use linear regression, some of our estimates might be
outside the [0, 1] interval, making them hard to interpret
as probabilities.

Another limitation is that this approach cannot be easily extended to
accommodate qualitative responses with more than two levels.

*  The logistic model

We would like a model that predicts the relationship between $Pr(Y=1|X)$ and $X$.

The logistic function $p:\mathbb{R}\to [0,1]$ is given by
\begin{equation}
p(X) = \frac{ e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}.
\end{equation}


Or equivalently

\begin{equation}
\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}.	
\end{equation}


* Some properties of the logistic equation

The differential equation $x' = ax$ can be considered as a simplistic model of
population growth when $a > 0$. The quantity $x(t)$ measures the population
of some species at time $t$. 

The assumption that leads to the differential equation is that the rate of growth of the population (namely, $dx/dt$) is directly
proportional to the size of the population. 

Of course, this naive assumption omits many circumstances that govern actual population growth, including,
for example, the fact that actual populations cannot increase without bound.


To take this restriction into account, we can make the following further
assumptions about the population model:

1. If the population is small, the growth rate remains directly proportional to the size of the population.
2. If the population grows too large, however, the growth rate becomes negative.

One differential equation that satisfies these assumptions is the logistic population growth model. This differential equation is

\begin{equation}
x' = ax \left(1 - \frac{x}{N}\right),
\end{equation}

or the simplified version when $N=1$ given by

\begin{equation}
x' = ax \left(1 - x\right).
\end{equation}

One can show that the solution of this ODE has the form of the logistic function $p(X)$.

* Odds and logit

The quantity

\begin{equation}
\frac{p(X)}{1-p(X)},
\end{equation}
is called the /odds/.

Its logarithm is called /log-odds/ or simply /logit/:
\begin{equation}
\ln\left(\frac{p(X)}{1-p(X)}\right).
\end{equation}


Then, the /logistic/ regression model has a /linear/ logit in the variable $X$.


* Estimating the Regression CoeÔ¨Écients

The coefficients are estimated by the method of maximum likelihood. 

Recall

Let $X_1, X_2, X_3, \ldots, X_n$ be a random sample from a distribution with a parameter 
$\theta$ (In general, $\theta$ might be a vector, $\theta=(\theta_1,\theta_2,\ldots, \theta_k)$.) 

Suppose that $x_1, x_2, \ldots, x_n$ are the observed values of $X_1, X_2,\ldots , X_n$. 

We define the likelihood function as the probability of the observed sample as a function of $\theta$:

\begin{equation}
L(x_1,\ldots, x_n ; \theta ) = P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n; \theta )
\end{equation}

A maximum likelihood estimate of $\theta$, is a value that maximises the likelihood function.


In the case of the logistic regression we have two vectors:
+ A vector of features $x_i$ 's .
+ A vector of observed classes $y_i$ 's.

And the associated probabilities:
+ A probability $p$ of one class if $y_i = 1$.
+ A probability $1-p$ of the other class if $y_i = 0$.

Then the likelihood function (in two variables) is given by

\begin{equation}
\displaystyle L(\beta_0,\beta_1) = \prod_{i=1}^{n} p(x_i)^{y_i}\left( 1- p(x_i) \right)^{1-y_i}.
\end{equation}

* Log likelihood for the logistic regression

\begin{equation}
\begin{split}
\ln(L) & = \sum_{i=1}^{n} y_i\left( \ln(p(x_i) \right) + \left(1-y_i\right)\ln\left(1 - p(x_i)\right)\\
& = \sum_{i=1}^{n}\ln\left(1 - p(x_i)\right) + \sum_{i=1}^{n} y_i \ln\left(\frac{p(x_i)}{1-p(x_i)}\right)\\
& = \sum_{i=1}^{n}\ln\left(\frac{1}{1 + e^{\beta_0 + \beta_1 x_i}}\right) + \sum_{i=1}^{n} y_i \left(\beta_0 + \beta_1 x_i\right)\\
& = - \sum_{i=1}^{n}\ln\left(1 + e^{\beta_0 + \beta_1 x_i}\right) + \sum_{i=1}^{n} y_i \left(\beta_0 + \beta_1 x_i\right).
\end{split}
\end{equation}

* Critical points

Then we have to solve the equations

\begin{equation}
\begin{split}
\frac{\partial \ln(L)}{\partial \beta_0} &= - \sum_{i=1}^{n} \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} + \sum_{i=1}^{n} y_i \\
&= \sum_{i=1}^{n} (y_i-p(x_i) )= 0, \\
\frac{\partial \ln(L)}{\partial \beta_1} &= - \sum_{i=1}^{n} \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} x_i + \sum_{i=1}^{n} y_ix_i\\
&= \sum_{i=1}^{n} (y_i-p(x_i))x_i = 0. \\
\end{split}
\end{equation}

Solutions to this system of equations are obtained using numerical methods.

* Multiple Logistic Regression

We now consider the problem of predicting a binary response using multiple
predictors. By analogy with the extension from simple to multiple linear
regression, we can generalise as follows: If $X = (X_1, \ldots, X_p)$ are $p$ predictor variables
then

\begin{equation}
p(X) = \frac{ e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{ 1 + e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}},
\end{equation}

since 

\begin{equation}
\ln\left(\frac{p(X)}{1-p(X)}\right)= \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p.
\end{equation}


* Logistic Regression with More Than Two Classes

If $Y$ can take on more than two values, say $k$ of them, we can still use logistic regression. Then each class 
$c$ in $0:(k-1)$, will have its own set of parameter $\beta_0^{(c)}$ and $\beta_1^{(c)}$, and predicted probabilities
\begin{equation}
\displaystyle \mbox{Pr}(Y = c | X = x) = \frac{e^{\beta_0^{(c)} + \beta_1^{(c)}x}}{\sum_{j=0}^{k-1} e^{\beta_0^{(j)} + \beta_1^{(j)}x}}
\end{equation}


* The glm in R

In R, the function =glm= is used to fit generalised linear models, specified by
     giving a symbolic description of the linear predictor 


* Summary

Let's start by recalling the last model we were trying to implement: the Logistic regression model.

+ It is a statistical technique to predict categorical outcome (binomial/multinomial) of the response variable $Y$.

+ The prediction of logistic regression is a value $p(X)$ which is
  interpreted as the probability of $Y = 1$, given certain values
  $X$. This is usually written as $P(Y=1 | X)$. Then the value of $p(X)$ is
  a number between $0$ and $1$.

+ For the independent variable $X$, we would like to adjust the
  coefficients $\beta_0$ and $\beta_1$ such that 
\[ 
p(X) = \frac{ e^{\beta_0 + \beta_1 X}}{ 1 + e^{\beta_0 + \beta_1 X}}.
\]


+ In the case of having more than one explanatory variable (Multiple
  logistic regression), then we have the log-odds
\[
\ln\left(\frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots \beta_n X_n
\]


* The example in R

We started with a code to simulating the data and we are thinking in the following practical problem, given the age and gender of some patients in a hospital,
we have to create a model that decides if the patient needs a certain treatment or not, based on some records.

+ Prepare the data
+ Train the logistic model
+ Asses the predictions
+ Apply the model.

* Example (simulation)

#+begin_src R
## Y: patients need treatment "yes" "not"
## gender: 0 female 1 male
## age: ages between 18 and 80

## Sim data base
gender  <-  sample(c(0,1), size = 1000, replace=TRUE)
age  <-  round(runif(1000, 18, 80))
xb  <-  -12 + 4*gender  + 0.2*age
p  <-  exp(xb)/(1+exp(xb))
y  <- rbinom(1000,size=1,prob = p)
df  <-  data.frame(gender,age, y)
#+end_src

* The glm function

The data df will be used to train the model
First we make sure that =y= is taken as a categorical variable.

#+begin_example R
df$y  <-  as.factor(df$y)
#+end_example

Then we fit the logistic regression using the =Generalised Linear
Models= function =glm=, using the binomial family.

#+begin_example R
mylfit <- glm(y ~ gender + age, data=df, family="binomial")
#+end_example


* Null and Residual deviance.


*Null deviance*: The null deviance tells us how well we can predict our
output only using the intercept. Smaller is better.


*Residual deviance*: The residual deviance tells us how well we can
predict our output using the intercept and our inputs. Smaller is
better. The bigger the difference between the null deviance and
residual deviance is, the more helpful our input variables were for
predicting the output variable.


* Predictions

  Then we can generate our predictions
=y2= (taking a threshold of p = 0.50) and compare them with the actual
values =y=.

#+begin_example R
## Predictions
p1  <- predict(mylfit, df, type="response")
df  <-  cbind(df,p1)
df$y2  <- ifelse(df$p1 >0.5, 1, 0)
#+end_example 


* Assess the model: Confusion matrix and Accuracy

One way to asses the model, is by means of the confusion matrix.
A confusion matrix is a summary of prediction results on a classification problem.
We can also compute the accuracy of the model as the ratio of correct predictions to total predictions made.

#+begin_example R
## Predictions
## Confusion matrix
CC <- table(df$y,df$y2)

## Classification accuracy 
(CC[1,1]+CC[2,2]) / sum(CC) * 100
#+end_example



* Interpretation of the coefficients

Apply the model to the following records

#+begin_example R
data2 <- data.frame(
"Patient_ID" = c("00102", "00023", "00427"),
"age" = c(50,50,65),
"gender" = c(0,1,0))


p1  <- predict(mylfit, data2, type="response")
data2 <- cbind(data2,p1)
data2$Treatment  <- ifelse(data2$p1 >0.5, 1, 0)
data2
#+end_example


*Interpretation of the  coefficients*:

\[  \beta_0 + \beta_1 * \mbox{gender} + \beta_2 * \mbox{age} \]

Compare 

\[ e^{\beta_1} \]

with the quotient

\[
\frac{
\frac{p(x_1)}{1 - p(x_1)}
}{
\frac{p(x_0)}{1 - p(x_0)}
}
\]

where $x_1$ is a record of given age and male, and $x_0$ is a record of a female of the same age.



One can read this as: the odds of a man to need the treatment is $e^{\beta_1}$ times higher than 
the odds of a woman to need the treatment.

A similar interpretation follows for $\beta_2$.

The /intercept/ $\beta_0$ on the other hand, is not always meaningful, since in our case
is the logit of a female of age 0. 

* Intercept

Here are some graphs that may help understand the effect of the intercept

#+begin_example
B0  <- -5
XX  <-  seq(-10,10, by = 0.02)
PP  <-  exp(B0 + 2.3* XX)/(1+ exp(B0 + 2.3*XX))
plot(XX,PP)
#+end_example

In this sense, adding an intercept can help spread out the probabilities.



* Logistic regression with one explanatory variable being categorical.

If one of the explanatory variables happens to be categorical, then 
what one should do, is create a dummy variable for each of the 
different levels. 

Fortunately, =glm= does it itself:


Question: 
What if in our original simulated data set we take the /gender/ variable as categorical?
 
#+begin_example R
for(i in 1:nrow(df)){
df$NEWVAR[i]  <- sample(c("uno","dos","tres"), 1)
}

df$NEWVAR <- as.factor(df$NEWVAR)
mylfit2 <- glm(y ~ gender + age + NEWVAR, 
                      data=df, family="binomial")

summary(mylfit2)
#+end_example

* Confidence intervals

Since the logistic model is a non linear transformation, computing the confidence intervals is not as straightforward.

It may be interesting for you to read the following discussions:


[[https://stats.stackexchange.com/questions/354098/calculating-confidence-intervals-for-a-logistic-regression]]


[[https://stats.stackexchange.com/questions/5304/why-is-there-a-difference-between-manually-calculating-a-logistic-regression-95]]

#+begin_example R
confint(mylfit2)
#+end_example


* K-nearest neighbourhood

** Logistic Regression for more than two Response Classes

The two-class logistic regression models discussed in the previous sections have multiple-class
extensions, but in practice they tend not to be used all that often. 
One of the reasons is that the method of discriminant analysis, is popular for multiple-class classiÔ¨Åcation.
Here some other examples of methods to tackle the problem of classification you may find in the literature:

+ Logistic regression.
+ Linear discriminant analysis.
+ K-nearest neighbours.


Here we won't go into the details of k-nearest neighbourhood algorithm, the goal is to have an informal 
discussion and see its implementation

The following picture explain's the fundamentals

https://miro.medium.com/max/720/0*34SajbTO2C5Lvigs.webp

** Normalisation function

Different variables have different scaling units, like weight in kg
and height in cm. We normalise each one of the variables using the
formula (x-min(x))/(max(x) ‚Äî min(x)) that we will also see in the
examples below. Now if you have one variable of 200kg and another with
50kg, after normalisation both will be represented by the value
between 0 and 1.

** Example: Iris.

#+begin_example R

## We will be working with the iris data set.
df <- iris
head(df)

nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
 
##Run normalisation on first 4 columns of dataset 
## because they are the predictors.
## Note the use of the lapply function

df_norm <- as.data.frame(lapply(df[,c(1:4)], nor))
 
summary(df_norm)

##extract testing set consisting of a 
## subset of 90% of the rows

ran <- sample(1:nrow(df), 0.9 * nrow(df)) 
df_train <- df_norm[ran,] 
df_train_category <- df[ran,5] 


df_test <- df_norm[-ran,]  
df_test_category <- df[-ran,5]


##load the package class
 library(class) 
##run knn function
 pr <- knn(df_train,df_test,cl=df_train_category,k=5)
 
 ##create confusion matrix
CC <- table(pr,df_test_category)
 
## accuracy is the ratio of  correct predictions 
## by total number of predictions 
 
 accuracy <- function(x){
                  sum(diag(x)/(sum(x,na.rm=TRUE))) * 100
		  }
 accuracy(CC)
#+end_example
