#+title: Lecture 4: Statistical inference
#+author: Daniel Ballesteros-Ch√°vez
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.1 (Org mode 9.3.6)
#+PROPERTY: header-args :R+ :exports both
#+PROPERTY: header-args :R+ :session *R*
#+HTML_HEAD: <style type="text/css"> tr:nth-child(odd) {background-color: #e2e2e2;}  tr:first-child {font-weight: bold}  tr:hover {background-color: #d0c6e5;}</style>
#+HTML_HEAD_EXTRA: <style>code {background-color: #ccc}</style>
:results:
#+HTML_HEAD:<style>
#+HTML_HEAD:/* Daniel Ballesteros-Chavez */
#+HTML_HEAD:/* DBCh CSS for blog project */
#+HTML_HEAD:/* color schemes: #333745; #E63462 ; #C7EFCF ; #EEF5DB ; #909396; #262626;*/
#+HTML_HEAD:/* Modified version with responsive TOC
#+HTML_HEAD:
#+HTML_HEAD:/* usage: #+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./style01.css"/> */
#+HTML_HEAD:
#+HTML_HEAD:body {
#+HTML_HEAD:	font-size: 18px;
#+HTML_HEAD:	color: #404040;
#+HTML_HEAD:	/* background-color: #333745; */
#+HTML_HEAD:	font-family: Helvetica;
#+HTML_HEAD:	line-height: 1.3;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#content {
#+HTML_HEAD:	max-width: 50em;
#+HTML_HEAD:	margin-left: auto;
#+HTML_HEAD:	margin-right: auto;
#+HTML_HEAD:    padding: 15px 50px 50px 15px;
#+HTML_HEAD:    background-color: #E4F7FF;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:p {
#+HTML_HEAD:		text-align: justify;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:/* this part is about the table of contents TOC */
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents a:link,
#+HTML_HEAD:#table-of-contents a:visited {
#+HTML_HEAD:    color: #404040;
#+HTML_HEAD:    background: transparent;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents a:hover {
#+HTML_HEAD:  background-color: #ccc;
#+HTML_HEAD:  color: #404040;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents {
#+HTML_HEAD:    line-height: 1.2;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents h2 {
#+HTML_HEAD:    background-color:  #ccc ;
#+HTML_HEAD:    padding-left: 0.3em;
#+HTML_HEAD:    color: #404040;
#+HTML_HEAD:    border-bottom: 0;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents ul {
#+HTML_HEAD:    list-style: none;
#+HTML_HEAD:    padding-left: 0.3em;
#+HTML_HEAD:    font-weight: normal;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents div>ul>li {
#+HTML_HEAD:    margin-top: 1em;
#+HTML_HEAD:    font-weight: bold;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents .tag {
#+HTML_HEAD:    display: none;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents .todo,
#+HTML_HEAD:#table-of-contents .done {
#+HTML_HEAD:    font-size: 80%;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:#table-of-contents ol>li {
#+HTML_HEAD:    margin-top: 1em;
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:@media screen {
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents {
#+HTML_HEAD:        position: fixed;
#+HTML_HEAD:        top: 0;
#+HTML_HEAD:        left: 0;
#+HTML_HEAD:        padding: 1em 0 1em 1em;
#+HTML_HEAD:        width: 290px;
#+HTML_HEAD:        height: 100vh;
#+HTML_HEAD:        overlow-x: hidden;
#+HTML_HEAD:        overlow-y: auto;
#+HTML_HEAD:	overflow: auto;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents h2 {
#+HTML_HEAD:        margin-top: 0;
#+HTML_HEAD:        font-family: Helvetica,Arial,"Lucida Grande",sans-serif;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents code {
#+HTML_HEAD:        font-size: 12px;
#+HTML_HEAD:    }
#+HTML_HEAD:    
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:@media screen and (max-width: 95em) {
#+HTML_HEAD:
#+HTML_HEAD:    #table-of-contents {
#+HTML_HEAD:        display: none;
#+HTML_HEAD:    }
#+HTML_HEAD:
#+HTML_HEAD:    h1.title {
#+HTML_HEAD:        margin-left: 0%;
#+HTML_HEAD:    }
#+HTML_HEAD:    
#+HTML_HEAD:    div#content {
#+HTML_HEAD:        margin-left: 5%;
#+HTML_HEAD:        max-width: 90%;
#+HTML_HEAD:    }
#+HTML_HEAD:}
#+HTML_HEAD:
#+HTML_HEAD:/*Html Boxes around THMs and Propositions */
#+HTML_HEAD:.abstract  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:	border: 1px solid #404040;
#+HTML_HEAD:    box-shadow: 3px 3px 3px ;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .abstract:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Abstract';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.mydef  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid #404040;
#+HTML_HEAD:    background-color: #FFD580;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px orange; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .mydef:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    /* background-color: white; */
#+HTML_HEAD:    background-color: orange;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Definition';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.prop  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid ;
#+HTML_HEAD:    background-color: #F1FFC2;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px green; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .prop:before {
#+HTML_HEAD:    	color:  white;
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: green;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Proposition';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:.thm  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid ;
#+HTML_HEAD:    background-color: lightcyan;
#+HTML_HEAD:    /* box-shadow: 3px 3px 3px brown; */
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .thm:before {
#+HTML_HEAD:    	color:  white;
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: darkblue;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Theorem';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:  .cor  {
#+HTML_HEAD:    	color:  #404040;
#+HTML_HEAD:    border: 1px solid blue;
#+HTML_HEAD:    box-shadow: 3px 3px 3px blue;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:	   }
#+HTML_HEAD:
#+HTML_HEAD:  .cor:before {
#+HTML_HEAD:    display: inline;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -5px;
#+HTML_HEAD:    left: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:    content: 'Corollary';
#+HTML_HEAD:  }
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:/*defaults form org-mode export */
#+HTML_HEAD:
#+HTML_HEAD:
#+HTML_HEAD:  .title  { text-align: center; }
#+HTML_HEAD:  .todo   { font-family: monospace; color: red; }
#+HTML_HEAD:  .done   { color: green; }
#+HTML_HEAD:  .tag    { background-color: #eee; font-family: monospace;
#+HTML_HEAD:            padding: 2px; font-size: 80%; font-weight: normal; }
#+HTML_HEAD:  .timestamp { color: #bebebe; }
#+HTML_HEAD:  .timestamp-kwd { color: #5f9ea0; }
#+HTML_HEAD:  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
#+HTML_HEAD:  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
#+HTML_HEAD:  .center { margin-left: auto; margin-right: auto; text-align: center; }
#+HTML_HEAD:  .underline { text-decoration: underline; }
#+HTML_HEAD:  #postamble p, #preamble p { font-size: 90%; margin: .2em; text-align: center;}
#+HTML_HEAD:  p.verse { margin-left: 3%; }
#+HTML_HEAD:  pre {
#+HTML_HEAD:    border: 1px solid #ccc;
#+HTML_HEAD:    box-shadow: 3px 3px 3px #eee;
#+HTML_HEAD:    padding: 8pt;
#+HTML_HEAD:    font-family: monospace;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    margin: 1.2em;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src {
#+HTML_HEAD:    position: relative;
#+HTML_HEAD:    overflow: auto;
#+HTML_HEAD:    padding-top: 1.2em;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src:before {
#+HTML_HEAD:    display: none;
#+HTML_HEAD:    position: absolute;
#+HTML_HEAD:    background-color: white;
#+HTML_HEAD:    top: -10px;
#+HTML_HEAD:    right: 10px;
#+HTML_HEAD:    padding: 3px;
#+HTML_HEAD:    border: 1px solid black;
#+HTML_HEAD:  }
#+HTML_HEAD:  pre.src:hover:before { display: inline;}
#+HTML_HEAD:  pre.src-sh:before    { content: 'sh'; }
#+HTML_HEAD:  pre.src-bash:before  { content: 'sh'; }
#+HTML_HEAD:  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
#+HTML_HEAD:  pre.src-R:before     { content: 'R'; }
#+HTML_HEAD:  pre.src-perl:before  { content: 'Perl'; }
#+HTML_HEAD:  pre.src-java:before  { content: 'Java'; }
#+HTML_HEAD:  pre.src-sql:before   { content: 'SQL'; }
#+HTML_HEAD:
#+HTML_HEAD:  table { border-collapse:collapse; }
#+HTML_HEAD:  caption.t-above { caption-side: top; }
#+HTML_HEAD:  caption.t-bottom { caption-side: bottom; }
#+HTML_HEAD:  td, th { vertical-align:top;  }
#+HTML_HEAD:  th.right  { text-align: center;  }
#+HTML_HEAD:  th.left   { text-align: center;   }
#+HTML_HEAD:  th.center { text-align: center; }
#+HTML_HEAD:  td.right  { text-align: right;  }
#+HTML_HEAD:  td.left   { text-align: left;   }
#+HTML_HEAD:  td.center { text-align: center; }
#+HTML_HEAD:  dt { font-weight: bold; }
#+HTML_HEAD:  .footpara:nth-child(2) { display: inline; }
#+HTML_HEAD:  .footpara { display: block; }
#+HTML_HEAD:  .footdef  { margin-bottom: 1em; }
#+HTML_HEAD:  .figure { padding: 1em; }
#+HTML_HEAD:  .figure p { text-align: center; }
#+HTML_HEAD:  .inlinetask {
#+HTML_HEAD:    padding: 10px;
#+HTML_HEAD:    border: 2px solid gray;
#+HTML_HEAD:    margin: 10px;
#+HTML_HEAD:    background: #ffffcc;
#+HTML_HEAD:  }
#+HTML_HEAD:  #org-div-home-and-up
#+HTML_HEAD:   { text-align: right; font-size: 70%; white-space: nowrap; }
#+HTML_HEAD:  textarea { overflow-x: auto; }
#+HTML_HEAD:  .linenr { font-size: smaller }
#+HTML_HEAD:  .code-highlighted { background-color: #ffff00; }
#+HTML_HEAD:  .org-info-js_info-navigation { border-style: none; }
#+HTML_HEAD:  #org-info-js_console-label
#+HTML_HEAD:    { font-size: 10px; font-weight: bold; white-space: nowrap; }
#+HTML_HEAD:  .org-info-js_search-highlight
#+HTML_HEAD:    { background-color: #ffff00; color: #000000; font-weight: bold; }
#+HTML_HEAD:
#+HTML_HEAD:</style>
:end:


* Introduction.

Statistical inference is the process of using data analysis to infer
properties of an underlying distribution of probability. Inferential
statistical analysis infers properties of a population, for example by
testing hypotheses and deriving estimates. It is assumed that the
observed data set is sampled from a larger population.

We also discuss the Law of Large Numbers and the Central Limit theorem, 
as well as the definition of Confidence Intervals for Mean estimators.


* Sampling and measurement error models

There are two standard paradigms for inference:

+ Sampling model
+ Measurement error model.


Two most common methods of statistical inference are:

+ Confidence intervals
+ Hypotesis testing
  + Paramtetric
  + Non paramtetric



* Estimators and Sampling Distributions

The collection of random variables $X_{1}, X_{2}, X_{3}, ..., X_{n}$ is said to be a random sample of size $n$
if they are independent and identically distributed (i.i.d.), i.e.,
$X_{1} , X_{2}, X_{3}, ..., X_{n}$ are independent random variables, and
they have the same distribution, i.e,
$F_{X_1}(x)=F_{X_{2}}(x)=...=F_{X_{n}}(x)$, for all $x\in \mathbb{R}$.


A *parameters* is any value that describe properties of the whole population.    
A function of the random sample $\{X_{1} , . . . , X_{n} \}$ which is used to estimate the value of
an unknown parameter is called an *estimator*. One can see estimators also as random variables with its own distribution called the *sampling distribution*, and this shows how an estimator would change if we draw repeatedly new samples


** Sampling distribution of the sample mean

Let us select a random sample $\{X_{1} , . . . , X_{n} \}$ from a population with mean $\mu$ and variance $\sigma^{2}$, in other words, let
$X_{1} , . . . , X_{n}$ are independent, identically distributed random variables with $E[X_{i} ] = \mu$ and $Var[X_{i} ] =\sigma^{2}$ .


An estimator for the population mean $\mu$ is given by 


\begin{equation}
\hat{Y} = \frac{1}{n}\Sigma_{i =1}^{n} X_{i} 
\end{equation}


Note that the expected value and the variance are

\begin{equation}
E[\hat{Y}] = \mu,
\end{equation}

\begin{equation}
Var[\hat{Y}]  = \sigma^{2} / n.
\end{equation}


The *standard error* is the standard deviation of the estimator, i.e. 
\begin{equation}
s.e. =  \sqrt{Var[\hat{Y}]} = \sigma / \sqrt{n}
\end{equation}


If  $X_{i} \sim N(\mu, \sigma^{2})$ for all $i = 1, 2, \ldots,n$,  then 
\begin{equation}
\hat{Y} \sim N\left(\mu , \frac{\sigma^{2}}{ n}\right).
\end{equation}


Even if the X_{i} have some other distribution, for large n, the distribution of ≈∂ is
approximately normal. This is known as the *Central Limit Theorem* and it is one of the
most famous facts of probability theory.

* The (Weak) Law of Large Numbers

It states that if you repeat an experiment independently a large number of times and average
the result, what you obtain should be close to the expected value.
#+begin_thm
Let $X_1, X_2, \ldots, X_{n}$ be i.i.d. random variables with finite expected value
$E[X_{i}] = \mu$. Then for any $\epsilon >0$, then
\begin{equation}
\lim_{n \to \infty}P \left(| \hat{Y} - \mu | \geq \epsilon \right) = 0.
\end{equation}
#+end_thm

This result can be viewd as a special case of the Chebyshev's inequality.


* The central limit theorem

The Central Limit Theorem of probability states that the sum of many
small independent random variables will be a random variable with an
approximate normal distribution.


For example, the heights of women in the United States follow an approximate
normal distribution. The Central Limit Theorem applies here because height is
affected by many small additive factors. In contrast, the distribution of heights
of all adults in the United States is not so close to normality. The Central Limit
Theorem does not apply here because there is a single large factor‚Äîsex‚Äîthat
represents much of the total variation.

Example 1. Uniform distribution. 

A. Take five uniformly distributed random numbers between 0 and 10 and work out the
average.

#+begin_src R
mean(runif(5,0,10))
#+end_src

#+RESULTS:
: 3.94939380558208

Typically, of course, the average will be close to 5.

B. Let us do this 10,000 times and look at the distribution of the 10,000
means.

#+begin_example R

#Distribution of the raw data
runif(10000,0,10))

#Distribution of the means
means <- numeric(10000)
for (i in 1:10000){
means[i] <- mean(runif(5,0,10))
}

hist(means)
#+end_example

Now let's take a look at the normal distribution

#+begin_example R
mm  <- mean(means)
ssdd <-  sd(means)
xx  <-  seq(0,10,0.1)
yy  <-  dnorm(xx, mm, ssdd)

plot(xx,yy)
#+end_example

The height that gives the right scale, depends on the chosen bin widths, note that if we doubled with width of the bin there would be (close to) twice
as many numbers in the bin and the bar would be in principle twice as high too. Then get the height of the bars
on our frequency scale we multiply the total frequency, 10 000 by the bin width, 0.5 to get 5000.

#+begin_example R
hist(means)
lines(xx,yy*5000)
#+end_example

* Bias of an estimator

If $\hat{Y}$ is an estimator of a parameter $Y$, then $\hat{Y}$ is called *unbiased* if $E[\hat{Y}] = Y$. In general, the value
$E[\hat{Y}] - Y$ is called the bias of the estimator and if it is different from zero, the estimator is said to be *biased*.


Example. For a random sample $\{X_{1} , . . . , X_{n} \}$ from a population with $E [X_{i} ] = \mu$ and $\mbox{Var}[X_{i} ] =\sigma^{2}$ .

Se have seen that the estimator
\begin{equation}
\hat{Y} = \Sigma_{i =1}^{n} X_{i} / n,
\end{equation}
is an unbiased estimator for the mean \mu.

Show that the sample variance

\begin{equation}
\hat{s}^2 = \frac{1}{n-1} \Sigma_{i=1}^{n} (X_{i} - \hat{Y})^{2},
\end{equation}

is an unbiased estimator of $\sigma^{2}$.

On the other hand, the sample standard deviation $\hat{s}$, is biased.
to see this, note that $\hat{s}$
 is random (non-degenerate), so $\mbox{Var}(\hat{s})>0$, and 
$\mbox{Var}(\hat{s}) = E[\hat{s}^{2}] - E[\hat{s}]^{2}$.


* Maximum Likelihood Estimation

Apart from the mean and variance estimators, it is not clear how we can estimate other parameters. 
We now would like to talk about a systematic way of parameter estimation. 
Specifically, we would like to introduce an estimation method, called maximum likelihood estimation (MLE). 

Example

I have a bag that contains 3 balls. Each ball is either red or blue,
but I have no information in addition to this. Thus, the number of
blue balls, call it $\theta$, might be 0, 1, 2, or 3. I am allowed to choose
4 balls at random from the bag with replacement. We define the random
variables $X_{1}, X_{2}, X_{3}$, and $X_{4}$
as follows:
$X_{i}= 1$ if the i-th chosen ball is blue, and $X_{i} = 0$ if it is red.
Note that $X{i}$'s are i.i.d. and $X_{i} \sim Bernoulli\left(\frac{\theta}{3}\right)$. Lets work out the most likely value for
$\theta$.


If after the experiment I obtain the values $X_{1} = 1, X_{2} = 0, X_{3}=1, X_{4} = 1$, i.e., three blues and two reds, then 
+ For each possible value of $\theta$, find the probability of the observed sample.
+ For which value of \theta is the probability of the sample the largest?


 Let $X_{1}, X_{2}, X_{3}, ..., X_{n}$ be a random sample from a distribution with a
 parameter $\theta$ (In general, $\theta$ might be a vector, $\theta=(\theta_{1},\theta_{2},\ldots,\theta_{k})$.)
 Suppose that $x_{1}, x_{2}, x_{3}, \ldots, x_{n}$ are the observed values of $X_{1}, X_{2}, X_{n}$. We define the
 likelihood function as the probability of the observed sample as a
 function of $\theta$:

\begin{equation}
L(x_{1},\ldots,x_{n} ; \theta ) = P(X_{1} = x_{1}, X_{2} = x_{2}, \ldots, X_{n} = x_{n};\theta )
\end{equation}

A maximum likelihood estimate of $\theta$, is a value of $\theta$ that maximises the likelihood function.

A maximum likelihood estimator (MLE) of the parameter $\theta$ is a random variable whose value when
$X_{i} = x_{i}$, equals the maximum likelihood estimate of $\theta$.


Examples:

For $X_{i} \sim Binom(3,\theta)$, and observed values $(x_{1}, x_{2}, x_{3}, x_{4}) = (1, 3, 2, 2)$, find the likelihood function, find the maximum
likelihood estimate of $\theta$


For $X_{i} \sim Exponential(\theta)$ and observed values $x_{1}, x_{2}, ..., x_{n}$, find the maximum likelihood estimator (MLE) of $\theta$


Generalised to several variables.

Example 

Suppose that we have observed the random sample $X_{1}, X_{2}, X_{3}, X_{n}$, where $X_{i} \sim N(Œ∏_{1},Œ∏_{2})$. Find the maximum likelihood
estimators for $Œ∏_{1}$ and $Œ∏_{2}$.



* Confidence intervals

Let $X_{1}, X_{2}, \ldots, X_{n}$ be a random sample from a distribution
with a parameter $\theta$ that is to be estimated. An interval estimator with
confidence level $1‚àí\alpha$ consists of two estimators such that

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;P\left(\hat{\Theta}_l&space;\leq&space;\theta&space;\leq&space;\hat{\Theta}_h&space;\right&space;)&space;\geq&space;1&space;-&space;\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;P\left(\hat{\Theta}_l&space;\leq&space;\theta&space;\leq&space;\hat{\Theta}_h&space;\right&space;)&space;\geq&space;1&space;-&space;\alpha" title="\Large\color{DarkBlue} P\left(\hat{\Theta}_l \leq \theta \leq \hat{\Theta}_h \right ) \geq 1 - \alpha" /></a>

\begin{equation}
P\left(\hat{\theta}_l\leq\theta\leq\hat{\theta}_h\right)\geq 1-\alpha
\end{equation}


Equivalently, we say that this is a $(1-\alpha)$ 100% confice interval for $\theta$.



Example

Let $Z\sim N(0,1)$, find $x_{l}$ and $x_{h}$ such that

\begin{equation}
P(x_{l} \leq  Z \leq x_{h})=0.95
\end{equation}


Solution: x_{l} = -1.96, and x_{h}  = 1.96


Example

Given a random sample $X_1, X_2, \ldots, X_n$ from a normal distribution $N(\theta, \sigma^2)$. Find the confidence interval for the parameter $\theta$ (i,e, the mean) with a confidence level of $(1-\alpha)$.

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;\left[&space;\hat{\theta}&space;-&space;z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}&space;\right]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;\left[&space;\hat{\theta}&space;-&space;z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}&space;\right]" title="\Large\color{DarkBlue} \left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \right]" /></a>

\begin{equation}
\left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \right]
\end{equation}


** Confidence intervals If we do not know the standard deviation \sigma 

So far we  have discussed how to get the Confidence Interval with a confidence level of (1-\alpha) when the parameter of the population we want to measure is the mean \theta of certain characteristic, using $\hat{\theta}$ as an estimator, assuming X_{i} ~ N(\Theta,\sigma^{2}), with a large sample size n.

Usually the problem is that we do not know the real value of \sigma. For this, we have two options:

+ We can try to find and estimate for the maximum value of \sigma:  0 \leq \sigma \leq max \sigma.
+ Use the sample variance estimator for the standar deviation.

In the first case, if we have 0 \leq \sigma \leq max \sigma, then it is clear that the interval

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;\left[&space;\hat{\theta}&space;-&space;z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}&space;\right]&space;\subseteq&space;\left[&space;\hat{\theta}&space;-&space;z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}}&space;\right]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;\left[&space;\hat{\theta}&space;-&space;z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}&space;\right]&space;\subseteq&space;\left[&space;\hat{\theta}&space;-&space;z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}}&space;\right]" title="\Large\color{DarkBlue} \left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \right] \subseteq \left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}} \right]" /></a>

\begin{equation}
\left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \right] \subseteq \left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}} \right]
\end{equation}

and hence the following inequality holds:

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;P\left(&space;\left[&space;\hat{\theta}&space;-&space;z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}}&space;\right]\right&space;)&space;\geq&space;(1-\alpha)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;P\left(&space;\left[&space;\hat{\theta}&space;-&space;z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}}&space;\right]\right&space;)&space;\geq&space;(1-\alpha)" title="\Large\color{DarkBlue} P\left( \left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}} \right]\right ) \geq (1-\alpha)" /></a>
\begin{equation}
P\left( \left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{\max\sigma}{\sqrt{n}} \right]\right ) \geq (1-\alpha)
\end{equation}

In the second case, one can estimate the variance using

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;S^2&space;=&space;\frac{1}{n-1}&space;\sum_{i=1}^{n}&space;\left(&space;X_i&space;-&space;\hat{\theta}\right&space;)^2&space;\\&space;=&space;\frac{1}{n-1}&space;\left(&space;\sum_{i=1}^{n}&space;X_i^2&space;-&space;n\hat{\theta}&space;^2&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;S^2&space;=&space;\frac{1}{n-1}&space;\sum_{i=1}^{n}&space;\left(&space;X_i&space;-&space;\hat{\theta}\right&space;)^2&space;\\&space;=&space;\frac{1}{n-1}&space;\left(&space;\sum_{i=1}^{n}&space;X_i^2&space;-&space;n\hat{\theta}&space;^2&space;\right&space;)" title="\Large\color{DarkBlue} S^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \hat{\theta}\right )^2 \\ = \frac{1}{n-1} \left( \sum_{i=1}^{n} X_i^2 - n\hat{\theta} ^2 \right )" /></a>

\begin{equation}
S^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \hat{\theta}\right )^2 = \frac{1}{n-1} \left( \sum_{i=1}^{n} X_i^2 - n\hat{\theta} ^2 \right )
\end{equation}

this is we can approximate the interval by solving
\begin{equation}
P\left( \left[ \hat{\theta} - z_{\frac{\alpha}{2}}\frac{S}{\sqrt{n}} , \hat{\theta} + z_{\frac{\alpha}{2}}\frac{S}{\sqrt{n}} \right]\right ) \geq (1-\alpha)
\end{equation}


** Confidence intervals if the sample size is small.

In this case, we cannot apply the Central Limit Theorem. 
We then relay on the following properties. Recall that we are interested
in estimating the population mean \mu using our sample estimator $\hat{\theta}$.

Then

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;\hat{\theta}&space;\sim&space;N(\mu,&space;\sigma^2/n),\\&space;\\&space;\frac{n-1}{\sigma^2}S^2&space;\sim&space;\chi^2_{n-1},\\&space;\\&space;\frac{\hat{\theta}-&space;\mu}{\sigma/\sqrt{n}}&space;\sim&space;t_{n-1}." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;\hat{\theta}&space;\sim&space;N(\mu,&space;\sigma^2/n),\\&space;\\&space;\frac{n-1}{\sigma^2}S^2&space;\sim&space;\chi^2_{n-1},\\&space;\\&space;\frac{\hat{\theta}-&space;\mu}{\sigma/\sqrt{n}}&space;\sim&space;t_{n-1}." title="\Large\color{DarkBlue} \hat{\theta} \sim N(\mu, \sigma^2/n),\\ \\ \frac{n-1}{\sigma^2}S^2 \sim \chi^2_{n-1},\\ \\ \frac{\hat{\theta}- \mu}{\sigma/\sqrt{n}} \sim t_{n-1}." /></a>



+ $\hat{\theta} \sim N(\mu, \sigma^2/n)$
+ $\frac{n-1}{\sigma^2}S^2 \sim \chi^2_{n-1}$
+ $\frac{\hat{\theta}- \mu}{\sigma/\sqrt{n}} \sim t_{n-1}$



Hence the confidence interval is 

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;\left[&space;\hat{\theta}&space;-&space;t_{n-1}\left({\frac{\alpha}{2}}&space;\right&space;)\frac{S}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;t_{n-1}\left(&space;{\frac{\alpha}{2}}\right&space;)\frac{S}{\sqrt{n}}&space;\right]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;\left[&space;\hat{\theta}&space;-&space;t_{n-1}\left({\frac{\alpha}{2}}&space;\right&space;)\frac{S}{\sqrt{n}}&space;,&space;\hat{\theta}&space;&plus;&space;t_{n-1}\left(&space;{\frac{\alpha}{2}}\right&space;)\frac{S}{\sqrt{n}}&space;\right]" title="\Large\color{DarkBlue} \left[ \hat{\theta} - t_{n-1}\left({\frac{\alpha}{2}} \right )\frac{S}{\sqrt{n}} , \hat{\theta} + t_{n-1}\left( {\frac{\alpha}{2}}\right )\frac{S}{\sqrt{n}} \right]" /></a>

\begin{equation}
\left[ \hat{\theta} - t_{n-1}\left({\frac{\alpha}{2}} \right )\frac{S}{\sqrt{n}} , \hat{\theta} + t_{n-1}\left( {\frac{\alpha}{2}}\right )\frac{S}{\sqrt{n}} \right]
\end{equation}


where $t_{n-1}(\alpha/2)$ is defined in a similar way to $z_{\alpha/2}$, using now the $t_{n-1}$ distribution.






# * Quick note on the Normal Distribution

# Today we did the Following discussed the following points.  Before
# getting into maters I wish to thank Paulina Ha≈Çatek for her valuable
# comments while discussing these ideas on the board.

# 1) We showed that 
# \begin{equation}
# \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}} dx = 1.
# \end{equation}
# First noted that
# \begin{equation}
# \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}} dx = 2\int_{0}^{\infty} e^{-\frac{x^2}{2}} dx.
# \end{equation}
# Then we defined
# \begin{equation}
# L =\int_{0}^{\infty} e^{-\frac{x^2}{2}} dx.
# \end{equation}
# Then we wrote
# \begin{equation}
# L^2 = \int_0^{\infty}\int_0^{\infty}e^{-\frac{x^2 + y^2}{2}} dxdy.
# \end{equation}
# We changed to polar coordinates $0\leq r <\infty$ and $0\leq \theta \leq \frac{\pi}{2}$ given by $r^2 = x^2 + y^2$,  $x = r\cos\theta$ $y = r\sin\theta$.
# Then by computing the Jacobian (using change of variable formula) we had
# \begin{equation}
# L^2 = \int_0^{\infty}\int_0^{\infty}e^{-\frac{x^2 + y^2}{2}} dxdy = \int_{0}^{\pi/2}\int_{0}^{\infty}e^{-r^2/2}rdrd\theta.
# \end{equation}
# From this follows immediately that 
# \begin{equation}
# L^2 = \frac{\pi}{2}.
# \end{equation}
# Then we have the value of $L$ and tracing back the integrals we obtained our desired result.

# 2) [@2]  We also showed that if $X \sim N(0,1)$ then the expected value is
#     indeed $\mu = 0$. Just noticed the change of variable $v = \frac{x^2}{2}$ imply that $dv = x\,dx$ and then a direct
#     computation gives
# \begin{equation}
# \begin{split}
# E[X] & = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} x e^{-\frac{x^2}{2}} dx \\
#  & = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-v} dv  \\
# & =  - \left. \frac{1}{\sqrt{2\pi}} e^{-v} \right|^{\infty}_{-\infty}
# & =  - \left. \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}\right|^{\infty}_{-\infty} 
# &= \frac{1}{\sqrt{2\pi}} (0 - 0).
# &= 0.
# \end{split}
# \end{equation}

# 3) [@3] We also gave some evidence that if $X \sim N(\mu, \sigma^2)$ then the change of variable.
# \begin{equation}
# Z = \frac{X - \mu}{\sigma}
# \end{equation}
# makes $Z \sim N(0,1)$.


# Be aware that this is not a proof and that the arguments we discussed in the lecture were very
# informal. For a rigorous proof one has to call the *Fundamental Theorem of Calculus* at some point.

# The ideas were as follows:
# If $X \sim N(\mu,\sigma^2)$ then we can write the probability density function as
# \begin{equation}
# f_{X}(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}.
# \end{equation}

# We are trying to obtain an expression for $f_{Z}(x)$, and for that we use the following interpretation
# \begin{equation}
# P(Z = x) = P\left(\frac{X - \mu}{\sigma} = x \right) = P(X = \sigma x + \mu) = f_{X}(\sigma x + \mu).
# \end{equation}

# The we want to write
# \begin{equation}
# P(Z = x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{x^2}{2}}.
# \end{equation}

# This is not the result yet, because recall that 
# \begin{equation}
# f_{X}(x) = \frac{dF}{dx}.
# \end{equation}

# To fix it we have to multiply by the right differential, 
# in this case, $d(\sigma x + \mu) = \sigma$. Hence

# The probability density function of $Z$ is given by
# \begin{equation}
# f_{Z}(x) = P(Z = x) d(\sigma x + \mu) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}.
# \end{equation}

# This is precisely the probability density function for the standard normal distribution $N(0,1)$.


# In the next lecture we will be discussin the Hypothesis testing procedure, and it is important that this chance of variable 
# is clear (passing from $N(\mu,\sigma^2)$ distribution to a $N(0,1)$ distrubution by the so called "Z" statistic:
# \begin{equation}
# Z = \frac{X - \mu}{\sigma}.
# \end{equation}


