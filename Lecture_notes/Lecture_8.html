<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-01-11 Wed 15:19 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>The logistic regression</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Daniel Ballesteros-Chávez" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style type="text/css"> tr:nth-child(odd) {background-color: #e2e2e2;}  tr:first-child {font-weight: bold}  tr:hover {background-color: #d0c6e5;}</style>
<style>
/* Daniel Ballesteros-Chavez */
/* DBCh CSS for blog project */
/* color schemes: #333745; #E63462 ; #C7EFCF ; #EEF5DB ; #909396; #262626;*/
/* Modified version with responsive TOC
/* usage: #+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./style01.css"/> */
body {
font-size: 18px;
color: #404040;
/* background-color: #333745; */
font-family: Helvetica;
line-height: 1.3;
}
#content {
max-width: 50em;
margin-left: auto;
margin-right: auto;
padding: 15px 50px 50px 15px;
background-color: #E4F7FF;
}
p {
text-align: justify;
}
/* this part is about the table of contents TOC */
#table-of-contents a:link,
#table-of-contents a:visited {
color: #404040;
background: transparent;
}
#table-of-contents a:hover {
background-color: #ccc;
color: #404040;
}
#table-of-contents {
line-height: 1.2;
}
#table-of-contents h2 {
background-color:  #ccc ;
padding-left: 0.3em;
color: #404040;
border-bottom: 0;
}
#table-of-contents ul {
list-style: none;
padding-left: 0.3em;
font-weight: normal;
}
#table-of-contents div>ul>li {
margin-top: 1em;
font-weight: bold;
}
#table-of-contents .tag {
display: none;
}
#table-of-contents .todo,
#table-of-contents .done {
font-size: 80%;
}
#table-of-contents ol>li {
margin-top: 1em;
}
@media screen {
#table-of-contents {
position: fixed;
top: 0;
left: 0;
padding: 1em 0 1em 1em;
width: 290px;
height: 100vh;
overlow-x: hidden;
overlow-y: auto;
overflow: auto;
}
#table-of-contents h2 {
margin-top: 0;
font-family: Helvetica,Arial,"Lucida Grande",sans-serif;
}
#table-of-contents code {
font-size: 12px;
}
}
@media screen and (max-width: 95em) {
#table-of-contents {
display: none;
}
h1.title {
margin-left: 0%;
}
div#content {
margin-left: 5%;
max-width: 90%;
}
}
/*Html Boxes around THMs and Propositions */
.abstract  {
color:  #404040;
border: 1px solid #404040;
box-shadow: 3px 3px 3px ;
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.abstract:before {
display: inline;
position: absolute;
background-color: white;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Abstract';
}
.mydef  {
color:  #404040;
border: 1px solid #404040;
background-color: #FFD580;
/* box-shadow: 3px 3px 3px orange; */
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.mydef:before {
display: inline;
position: absolute;
/* background-color: white; */
background-color: orange;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Definition';
}
.prop  {
color:  #404040;
border: 1px solid ;
background-color: #F1FFC2;
/* box-shadow: 3px 3px 3px green; */
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.prop:before {
color:  white;
display: inline;
position: absolute;
background-color: green;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Proposition';
}
.thm  {
color:  #404040;
border: 1px solid ;
background-color: lightcyan;
/* box-shadow: 3px 3px 3px brown; */
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.thm:before {
color:  white;
display: inline;
position: absolute;
background-color: darkblue;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Theorem';
}
.cor  {
color:  #404040;
border: 1px solid blue;
box-shadow: 3px 3px 3px blue;
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.cor:before {
display: inline;
position: absolute;
background-color: white;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Corollary';
}
/*defaults form org-mode export */
.title  { text-align: center; }
.todo   { font-family: monospace; color: red; }
.done   { color: green; }
.tag    { background-color: #eee; font-family: monospace;
padding: 2px; font-size: 80%; font-weight: normal; }
.timestamp { color: #bebebe; }
.timestamp-kwd { color: #5f9ea0; }
.right  { margin-left: auto; margin-right: 0px;  text-align: right; }
.left   { margin-left: 0px;  margin-right: auto; text-align: left; }
.center { margin-left: auto; margin-right: auto; text-align: center; }
.underline { text-decoration: underline; }
#postamble p, #preamble p { font-size: 90%; margin: .2em; text-align: center;}
p.verse { margin-left: 3%; }
pre {
border: 1px solid #ccc;
box-shadow: 3px 3px 3px #eee;
padding: 8pt;
font-family: monospace;
overflow: auto;
margin: 1.2em;
}
pre.src {
position: relative;
overflow: auto;
padding-top: 1.2em;
}
pre.src:before {
display: none;
position: absolute;
background-color: white;
top: -10px;
right: 10px;
padding: 3px;
border: 1px solid black;
}
pre.src:hover:before { display: inline;}
pre.src-sh:before    { content: 'sh'; }
pre.src-bash:before  { content: 'sh'; }
pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
pre.src-R:before     { content: 'R'; }
pre.src-perl:before  { content: 'Perl'; }
pre.src-java:before  { content: 'Java'; }
pre.src-sql:before   { content: 'SQL'; }
table { border-collapse:collapse; }
caption.t-above { caption-side: top; }
caption.t-bottom { caption-side: bottom; }
td, th { vertical-align:top;  }
th.right  { text-align: center;  }
th.left   { text-align: center;   }
th.center { text-align: center; }
td.right  { text-align: right;  }
td.left   { text-align: left;   }
td.center { text-align: center; }
dt { font-weight: bold; }
.footpara:nth-child(2) { display: inline; }
.footpara { display: block; }
.footdef  { margin-bottom: 1em; }
.figure { padding: 1em; }
.figure p { text-align: center; }
.inlinetask {
padding: 10px;
border: 2px solid gray;
margin: 10px;
background: #ffffcc;
}
#org-div-home-and-up
{ text-align: right; font-size: 70%; white-space: nowrap; }
textarea { overflow-x: auto; }
.linenr { font-size: smaller }
.code-highlighted { background-color: #ffff00; }
.org-info-js_info-navigation { border-style: none; }
#org-info-js_console-label
{ font-size: 10px; font-weight: bold; white-space: nowrap; }
.org-info-js_search-highlight
{ background-color: #ffff00; color: #000000; font-weight: bold; }
</style>
<style>code {background-color: #ccc}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">The logistic regression</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org01ce52d">1. Introduction</a></li>
<li><a href="#orgb0f3521">2. Classification and the difficulties with linear regression approach</a></li>
<li><a href="#org52dcb91">3. Binary case</a></li>
<li><a href="#org0114a5f">4. The logistic model</a></li>
<li><a href="#orgd8a3ff7">5. Some properties of the logistic equation</a></li>
<li><a href="#orgf4608ca">6. Odds and logit</a></li>
<li><a href="#orgdbfeb65">7. Estimating the Regression Coeﬃcients</a></li>
<li><a href="#orgb66a1cc">8. Log likelihood for the logistic regression</a></li>
<li><a href="#org32ff803">9. Critical points</a></li>
<li><a href="#orgb02da54">10. Multiple Logistic Regression</a></li>
<li><a href="#orgd90f6b5">11. Logistic Regression with More Than Two Classes</a></li>
<li><a href="#orgc7c25a4">12. The glm in R</a></li>
<li><a href="#orge1fba8e">13. Summary</a></li>
<li><a href="#orga658d83">14. The example in R</a></li>
<li><a href="#org1252452">15. Example (simulation)</a></li>
<li><a href="#org4958166">16. The glm function</a></li>
<li><a href="#org32e19af">17. Null and Residual deviance.</a></li>
<li><a href="#org74e1614">18. Predictions</a></li>
<li><a href="#org9b636fc">19. Assess the model: Confusion matrix and Accuracy</a></li>
<li><a href="#org6db8e69">20. Interpretation of the coefficients</a></li>
<li><a href="#org8296cb2">21. Intercept</a></li>
<li><a href="#orgf8f8cd0">22. Logistic regression with one explanatory variable being categorical.</a></li>
<li><a href="#org86e3b59">23. Confidence intervals</a></li>
<li><a href="#org662f880">24. K-nearest neighbourhood</a>
<ul>
<li><a href="#org2a7b79f">24.1. Logistic Regression for more than two Response Classes</a></li>
<li><a href="#orge8adacd">24.2. Normalisation function</a></li>
<li><a href="#orgda6f8c5">24.3. Example: Iris.</a></li>
</ul>
</li>
</ul>
</div>
</div>



<div id="outline-container-org01ce52d" class="outline-2">
<h2 id="org01ce52d"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
The linear regression model discussed in previous lectures assumes that the response variable \(Y\) is quantitative. But in many situations, the response
variable is instead qualitative.
</p>

<p>
Often qualitative variables are referred to as <i>categorical</i>.
</p>

<p>
Then, the problem of predicting in this case is a problem of <b>classification</b>.
</p>
</div>
</div>

<div id="outline-container-orgb0f3521" class="outline-2">
<h2 id="orgb0f3521"><span class="section-number-2">2</span> Classification and the difficulties with linear regression approach</h2>
<div class="outline-text-2" id="text-2">
<p>
Some difficulties arise when we try to use a simple linear regression model in 
order to classify some data into some categories.
</p>

<p>
In the best scenario, the response variable’s values take on a natural ordering, such as
mild, moderate, and severe, and hopefully we also have that the gap between mild and moderate
is similar to the gap between moderate and severe, then a 1, 2, 3 coding
would be reasonable. 
</p>

<p>
Unfortunately, in general there is no natural way to
convert a qualitative response variable with more than two levels into a
quantitative response that is ready for linear regression.
</p>


<p>
For a binary (two level) qualitative response, the situation is better. It does not matter
if we flip the coding we use for the values of the \(Y\) -variable, the linear regression will
produce the same predictors.
</p>
</div>
</div>


<div id="outline-container-org52dcb91" class="outline-2">
<h2 id="org52dcb91"><span class="section-number-2">3</span> Binary case</h2>
<div class="outline-text-2" id="text-3">
<p>
For a binary response with a 0/1 coding as above, regression by least
squares does make sense; it can be shown that the estimate \(\hat{\beta}X\) obtained using linear
regression is in fact an estimate of \(Pr( Y=y |X)\) in this special
case. However, if we use linear regression, some of our estimates might be
outside the [0, 1] interval, making them hard to interpret
as probabilities.
</p>

<p>
Another limitation is that this approach cannot be easily extended to
accommodate qualitative responses with more than two levels.
</p>
</div>
</div>

<div id="outline-container-org0114a5f" class="outline-2">
<h2 id="org0114a5f"><span class="section-number-2">4</span> The logistic model</h2>
<div class="outline-text-2" id="text-4">
<p>
We would like a model that predicts the relationship between \(Pr(Y=1|X)\) and \(X\).
</p>

<p>
The logistic function \(p:\mathbb{R}\to [0,1]\) is given by
</p>
\begin{equation}
p(X) = \frac{ e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}.
\end{equation}


<p>
Or equivalently
</p>

\begin{equation}
\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}.	
\end{equation}
</div>
</div>


<div id="outline-container-orgd8a3ff7" class="outline-2">
<h2 id="orgd8a3ff7"><span class="section-number-2">5</span> Some properties of the logistic equation</h2>
<div class="outline-text-2" id="text-5">
<p>
The differential equation \(x' = ax\) can be considered as a simplistic model of
population growth when \(a > 0\). The quantity \(x(t)\) measures the population
of some species at time \(t\). 
</p>

<p>
The assumption that leads to the differential equation is that the rate of growth of the population (namely, \(dx/dt\)) is directly
proportional to the size of the population. 
</p>

<p>
Of course, this naive assumption omits many circumstances that govern actual population growth, including,
for example, the fact that actual populations cannot increase without bound.
</p>


<p>
To take this restriction into account, we can make the following further
assumptions about the population model:
</p>

<ol class="org-ol">
<li>If the population is small, the growth rate remains directly proportional to the size of the population.</li>
<li>If the population grows too large, however, the growth rate becomes negative.</li>
</ol>

<p>
One differential equation that satisfies these assumptions is the logistic population growth model. This differential equation is
</p>

\begin{equation}
x' = ax \left(1 - \frac{x}{N}\right),
\end{equation}

<p>
or the simplified version when \(N=1\) given by
</p>

\begin{equation}
x' = ax \left(1 - x\right).
\end{equation}

<p>
One can show that the solution of this ODE has the form of the logistic function \(p(X)\).
</p>
</div>
</div>

<div id="outline-container-orgf4608ca" class="outline-2">
<h2 id="orgf4608ca"><span class="section-number-2">6</span> Odds and logit</h2>
<div class="outline-text-2" id="text-6">
<p>
The quantity
</p>

\begin{equation}
\frac{p(X)}{1-p(X)},
\end{equation}
<p>
is called the <i>odds</i>.
</p>

<p>
Its logarithm is called <i>log-odds</i> or simply <i>logit</i>:
</p>
\begin{equation}
\ln\left(\frac{p(X)}{1-p(X)}\right).
\end{equation}


<p>
Then, the <i>logistic</i> regression model has a <i>linear</i> logit in the variable \(X\).
</p>
</div>
</div>


<div id="outline-container-orgdbfeb65" class="outline-2">
<h2 id="orgdbfeb65"><span class="section-number-2">7</span> Estimating the Regression Coeﬃcients</h2>
<div class="outline-text-2" id="text-7">
<p>
The coefficients are estimated by the method of maximum likelihood. 
</p>

<p>
Recall
</p>

<p>
Let \(X_1, X_2, X_3, \ldots, X_n\) be a random sample from a distribution with a parameter 
\(\theta\) (In general, \(\theta\) might be a vector, \(\theta=(\theta_1,\theta_2,\ldots, \theta_k)\).) 
</p>

<p>
Suppose that \(x_1, x_2, \ldots, x_n\) are the observed values of \(X_1, X_2,\ldots , X_n\). 
</p>

<p>
We define the likelihood function as the probability of the observed sample as a function of \(\theta\):
</p>

\begin{equation}
L(x_1,\ldots, x_n ; \theta ) = P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n; \theta )
\end{equation}

<p>
A maximum likelihood estimate of \(\theta\), is a value that maximises the likelihood function.
</p>


<p>
In the case of the logistic regression we have two vectors:
</p>
<ul class="org-ul">
<li>A vector of features \(x_i\) 's .</li>
<li>A vector of observed classes \(y_i\) 's.</li>
</ul>

<p>
And the associated probabilities:
</p>
<ul class="org-ul">
<li>A probability \(p\) of one class if \(y_i = 1\).</li>
<li>A probability \(1-p\) of the other class if \(y_i = 0\).</li>
</ul>

<p>
Then the likelihood function (in two variables) is given by
</p>

\begin{equation}
\displaystyle L(\beta_0,\beta_1) = \prod_{i=1}^{n} p(x_i)^{y_i}\left( 1- p(x_i) \right)^{1-y_i}.
\end{equation}
</div>
</div>

<div id="outline-container-orgb66a1cc" class="outline-2">
<h2 id="orgb66a1cc"><span class="section-number-2">8</span> Log likelihood for the logistic regression</h2>
<div class="outline-text-2" id="text-8">
\begin{equation}
\begin{split}
\ln(L) & = \sum_{i=1}^{n} y_i\left( \ln(p(x_i) \right) + \left(1-y_i\right)\ln\left(1 - p(x_i)\right)\\
& = \sum_{i=1}^{n}\ln\left(1 - p(x_i)\right) + \sum_{i=1}^{n} y_i \ln\left(\frac{p(x_i)}{1-p(x_i)}\right)\\
& = \sum_{i=1}^{n}\ln\left(\frac{1}{1 + e^{\beta_0 + \beta_1 x_i}}\right) + \sum_{i=1}^{n} y_i \left(\beta_0 + \beta_1 x_i\right)\\
& = - \sum_{i=1}^{n}\ln\left(1 + e^{\beta_0 + \beta_1 x_i}\right) + \sum_{i=1}^{n} y_i \left(\beta_0 + \beta_1 x_i\right).
\end{split}
\end{equation}
</div>
</div>

<div id="outline-container-org32ff803" class="outline-2">
<h2 id="org32ff803"><span class="section-number-2">9</span> Critical points</h2>
<div class="outline-text-2" id="text-9">
<p>
Then we have to solve the equations
</p>

\begin{equation}
\begin{split}
\frac{\partial \ln(L)}{\partial \beta_0} &= - \sum_{i=1}^{n} \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} + \sum_{i=1}^{n} y_i \\
&= \sum_{i=1}^{n} (y_i-p(x_i) )= 0, \\
\frac{\partial \ln(L)}{\partial \beta_1} &= - \sum_{i=1}^{n} \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} x_i + \sum_{i=1}^{n} y_ix_i\\
&= \sum_{i=1}^{n} (y_i-p(x_i))x_i = 0. \\
\end{split}
\end{equation}

<p>
Solutions to this system of equations are obtained using numerical methods.
</p>
</div>
</div>

<div id="outline-container-orgb02da54" class="outline-2">
<h2 id="orgb02da54"><span class="section-number-2">10</span> Multiple Logistic Regression</h2>
<div class="outline-text-2" id="text-10">
<p>
We now consider the problem of predicting a binary response using multiple
predictors. By analogy with the extension from simple to multiple linear
regression, we can generalise as follows: If \(X = (X_1, \ldots, X_p)\) are \(p\) predictor variables
then
</p>

\begin{equation}
p(X) = \frac{ e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{ 1 + e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}},
\end{equation}

<p>
since 
</p>

\begin{equation}
\ln\left(\frac{p(X)}{1-p(X)}\right)= \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p.
\end{equation}
</div>
</div>


<div id="outline-container-orgd90f6b5" class="outline-2">
<h2 id="orgd90f6b5"><span class="section-number-2">11</span> Logistic Regression with More Than Two Classes</h2>
<div class="outline-text-2" id="text-11">
<p>
If \(Y\) can take on more than two values, say \(k\) of them, we can still use logistic regression. Then each class 
\(c\) in \(0:(k-1)\), will have its own set of parameter \(\beta_0^{(c)}\) and \(\beta_1^{(c)}\), and predicted probabilities
</p>
\begin{equation}
\displaystyle \mbox{Pr}(Y = c | X = x) = \frac{e^{\beta_0^{(c)} + \beta_1^{(c)}x}}{\sum_{j=0}^{k-1} e^{\beta_0^{(j)} + \beta_1^{(j)}x}}
\end{equation}
</div>
</div>


<div id="outline-container-orgc7c25a4" class="outline-2">
<h2 id="orgc7c25a4"><span class="section-number-2">12</span> The glm in R</h2>
<div class="outline-text-2" id="text-12">
<p>
In R, the function <code>glm</code> is used to fit generalised linear models, specified by
     giving a symbolic description of the linear predictor 
</p>
</div>
</div>


<div id="outline-container-orge1fba8e" class="outline-2">
<h2 id="orge1fba8e"><span class="section-number-2">13</span> Summary</h2>
<div class="outline-text-2" id="text-13">
<p>
Let's start by recalling the last model we were trying to implement: the Logistic regression model.
</p>

<ul class="org-ul">
<li>It is a statistical technique to predict categorical outcome (binomial/multinomial) of the response variable \(Y\).</li>

<li>The prediction of logistic regression is a value \(p(X)\) which is
interpreted as the probability of \(Y = 1\), given certain values
\(X\). This is usually written as \(P(Y=1 | X)\). Then the value of \(p(X)\) is
a number between \(0\) and \(1\).</li>

<li>For the independent variable \(X\), we would like to adjust the
coefficients \(\beta_0\) and \(\beta_1\) such that</li>
</ul>
<p>
\[ 
p(X) = \frac{ e^{\beta_0 + \beta_1 X}}{ 1 + e^{\beta_0 + \beta_1 X}}.
\]
</p>


<ul class="org-ul">
<li>In the case of having more than one explanatory variable (Multiple
logistic regression), then we have the log-odds</li>
</ul>
<p>
\[
\ln\left(\frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots \beta_n X_n
\]
</p>
</div>
</div>


<div id="outline-container-orga658d83" class="outline-2">
<h2 id="orga658d83"><span class="section-number-2">14</span> The example in R</h2>
<div class="outline-text-2" id="text-14">
<p>
We started with a code to simulating the data and we are thinking in the following practical problem, given the age and gender of some patients in a hospital,
we have to create a model that decides if the patient needs a certain treatment or not, based on some records.
</p>

<ul class="org-ul">
<li>Prepare the data</li>
<li>Train the logistic model</li>
<li>Asses the predictions</li>
<li>Apply the model.</li>
</ul>
</div>
</div>

<div id="outline-container-org1252452" class="outline-2">
<h2 id="org1252452"><span class="section-number-2">15</span> Example (simulation)</h2>
<div class="outline-text-2" id="text-15">
<div class="org-src-container">
<pre class="src src-R"><span style="color: #795548;">## </span><span style="color: #795548;">Y: patients need treatment "yes" "not"</span>
<span style="color: #795548;">## </span><span style="color: #795548;">gender: 0 female 1 male</span>
<span style="color: #795548;">## </span><span style="color: #795548;">age: ages between 18 and 80</span>

<span style="color: #795548;">## </span><span style="color: #795548;">Sim data base</span>
gender  <span style="color: #0288D1;">&lt;-</span>  sample(c(0,1), size = 1000, replace=<span style="color: #3f51b5;">TRUE</span>)
age  <span style="color: #0288D1;">&lt;-</span>  round(runif(1000, 18, 80))
xb  <span style="color: #0288D1;">&lt;-</span>  -12 + 4*gender  + 0.2*age
p  <span style="color: #0288D1;">&lt;-</span>  exp(xb)/(1+exp(xb))
y  <span style="color: #0288D1;">&lt;-</span> rbinom(1000,size=1,prob = p)
df  <span style="color: #0288D1;">&lt;-</span>  data.frame(gender,age, y)
</pre>
</div>
</div>
</div>

<div id="outline-container-org4958166" class="outline-2">
<h2 id="org4958166"><span class="section-number-2">16</span> The glm function</h2>
<div class="outline-text-2" id="text-16">
<p>
The data df will be used to train the model
First we make sure that <code>y</code> is taken as a categorical variable.
</p>

<pre class="example">
df$y  &lt;-  as.factor(df$y)
</pre>

<p>
Then we fit the logistic regression using the <code>Generalised Linear
Models</code> function <code>glm</code>, using the binomial family.
</p>

<pre class="example">
mylfit &lt;- glm(y ~ gender + age, data=df, family="binomial")
</pre>
</div>
</div>


<div id="outline-container-org32e19af" class="outline-2">
<h2 id="org32e19af"><span class="section-number-2">17</span> Null and Residual deviance.</h2>
<div class="outline-text-2" id="text-17">
<p>
<b>Null deviance</b>: The null deviance tells us how well we can predict our
output only using the intercept. Smaller is better.
</p>


<p>
<b>Residual deviance</b>: The residual deviance tells us how well we can
predict our output using the intercept and our inputs. Smaller is
better. The bigger the difference between the null deviance and
residual deviance is, the more helpful our input variables were for
predicting the output variable.
</p>
</div>
</div>


<div id="outline-container-org74e1614" class="outline-2">
<h2 id="org74e1614"><span class="section-number-2">18</span> Predictions</h2>
<div class="outline-text-2" id="text-18">
<p>
  Then we can generate our predictions
<code>y2</code> (taking a threshold of p = 0.50) and compare them with the actual
values <code>y</code>.
</p>

<pre class="example">
## Predictions
p1  &lt;- predict(mylfit, df, type="response")
df  &lt;-  cbind(df,p1)
df$y2  &lt;- ifelse(df$p1 &gt;0.5, 1, 0)
</pre>
</div>
</div>


<div id="outline-container-org9b636fc" class="outline-2">
<h2 id="org9b636fc"><span class="section-number-2">19</span> Assess the model: Confusion matrix and Accuracy</h2>
<div class="outline-text-2" id="text-19">
<p>
One way to asses the model, is by means of the confusion matrix.
A confusion matrix is a summary of prediction results on a classification problem.
We can also compute the accuracy of the model as the ratio of correct predictions to total predictions made.
</p>

<pre class="example">
## Predictions
## Confusion matrix
CC &lt;- table(df$y,df$y2)

## Classification accuracy 
(CC[1,1]+CC[2,2]) / sum(CC) * 100
</pre>
</div>
</div>



<div id="outline-container-org6db8e69" class="outline-2">
<h2 id="org6db8e69"><span class="section-number-2">20</span> Interpretation of the coefficients</h2>
<div class="outline-text-2" id="text-20">
<p>
Apply the model to the following records
</p>

<pre class="example">
data2 &lt;- data.frame(
"Patient_ID" = c("00102", "00023", "00427"),
"age" = c(50,50,65),
"gender" = c(0,1,0))


p1  &lt;- predict(mylfit, data2, type="response")
data2 &lt;- cbind(data2,p1)
data2$Treatment  &lt;- ifelse(data2$p1 &gt;0.5, 1, 0)
data2
</pre>


<p>
<b>Interpretation of the  coefficients</b>:
</p>

<p>
\[  \beta_0 + \beta_1 * \mbox{gender} + \beta_2 * \mbox{age} \]
</p>

<p>
Compare 
</p>

<p>
\[ e^{\beta_1} \]
</p>

<p>
with the quotient
</p>

<p>
\[
\frac{
\frac{p(x_1)}{1 - p(x_1)}
}{
\frac{p(x_0)}{1 - p(x_0)}
}
\]
</p>

<p>
where \(x_1\) is a record of given age and male, and \(x_0\) is a record of a female of the same age.
</p>



<p>
One can read this as: the odds of a man to need the treatment is \(e^{\beta_1}\) times higher than 
the odds of a woman to need the treatment.
</p>

<p>
A similar interpretation follows for \(\beta_2\).
</p>

<p>
The <i>intercept</i> \(\beta_0\) on the other hand, is not always meaningful, since in our case
is the logit of a female of age 0. 
</p>
</div>
</div>

<div id="outline-container-org8296cb2" class="outline-2">
<h2 id="org8296cb2"><span class="section-number-2">21</span> Intercept</h2>
<div class="outline-text-2" id="text-21">
<p>
Here are some graphs that may help understand the effect of the intercept
</p>

<pre class="example">
B0  &lt;- -5
XX  &lt;-  seq(-10,10, by = 0.02)
PP  &lt;-  exp(B0 + 2.3* XX)/(1+ exp(B0 + 2.3*XX))
plot(XX,PP)
</pre>

<p>
In this sense, adding an intercept can help spread out the probabilities.
</p>
</div>
</div>



<div id="outline-container-orgf8f8cd0" class="outline-2">
<h2 id="orgf8f8cd0"><span class="section-number-2">22</span> Logistic regression with one explanatory variable being categorical.</h2>
<div class="outline-text-2" id="text-22">
<p>
If one of the explanatory variables happens to be categorical, then 
what one should do, is create a dummy variable for each of the 
different levels. 
</p>

<p>
Fortunately, <code>glm</code> does it itself:
</p>


<p>
Question: 
What if in our original simulated data set we take the <i>gender</i> variable as categorical?
</p>

<pre class="example">
for(i in 1:nrow(df)){
df$NEWVAR[i]  &lt;- sample(c("uno","dos","tres"), 1)
}

df$NEWVAR &lt;- as.factor(df$NEWVAR)
mylfit2 &lt;- glm(y ~ gender + age + NEWVAR, 
                      data=df, family="binomial")

summary(mylfit2)
</pre>
</div>
</div>

<div id="outline-container-org86e3b59" class="outline-2">
<h2 id="org86e3b59"><span class="section-number-2">23</span> Confidence intervals</h2>
<div class="outline-text-2" id="text-23">
<p>
Since the logistic model is a non linear transformation, computing the confidence intervals is not as straightforward.
</p>

<p>
It may be interesting for you to read the following discussions:
</p>


<p>
<a href="https://stats.stackexchange.com/questions/354098/calculating-confidence-intervals-for-a-logistic-regression">https://stats.stackexchange.com/questions/354098/calculating-confidence-intervals-for-a-logistic-regression</a>
</p>


<p>
<a href="https://stats.stackexchange.com/questions/5304/why-is-there-a-difference-between-manually-calculating-a-logistic-regression-95">https://stats.stackexchange.com/questions/5304/why-is-there-a-difference-between-manually-calculating-a-logistic-regression-95</a>
</p>

<pre class="example">
confint(mylfit2)
</pre>
</div>
</div>


<div id="outline-container-org662f880" class="outline-2">
<h2 id="org662f880"><span class="section-number-2">24</span> K-nearest neighbourhood</h2>
<div class="outline-text-2" id="text-24">
</div>
<div id="outline-container-org2a7b79f" class="outline-3">
<h3 id="org2a7b79f"><span class="section-number-3">24.1</span> Logistic Regression for more than two Response Classes</h3>
<div class="outline-text-3" id="text-24-1">
<p>
The two-class logistic regression models discussed in the previous sections have multiple-class
extensions, but in practice they tend not to be used all that often. 
One of the reasons is that the method of discriminant analysis, is popular for multiple-class classiﬁcation.
Here some other examples of methods to tackle the problem of classification you may find in the literature:
</p>

<ul class="org-ul">
<li>Logistic regression.</li>
<li>Linear discriminant analysis.</li>
<li>K-nearest neighbours.</li>
</ul>


<p>
Here we won't go into the details of k-nearest neighbourhood algorithm, the goal is to have an informal 
discussion and see its implementation
</p>

<p>
The following picture explain's the fundamentals
</p>

<p>
<a href="https://miro.medium.com/max/720/0*34SajbTO2C5Lvigs.webp">https://miro.medium.com/max/720/0*34SajbTO2C5Lvigs.webp</a>
</p>
</div>
</div>

<div id="outline-container-orge8adacd" class="outline-3">
<h3 id="orge8adacd"><span class="section-number-3">24.2</span> Normalisation function</h3>
<div class="outline-text-3" id="text-24-2">
<p>
Different variables have different scaling units, like weight in kg
and height in cm. We normalise each one of the variables using the
formula (x-min(x))/(max(x) — min(x)) that we will also see in the
examples below. Now if you have one variable of 200kg and another with
50kg, after normalisation both will be represented by the value
between 0 and 1.
</p>
</div>
</div>

<div id="outline-container-orgda6f8c5" class="outline-3">
<h3 id="orgda6f8c5"><span class="section-number-3">24.3</span> Example: Iris.</h3>
<div class="outline-text-3" id="text-24-3">
<pre class="example">

## We will be working with the iris data set.
df &lt;- iris
head(df)

nor &lt;-function(x) { (x -min(x))/(max(x)-min(x))   }
 
##Run normalisation on first 4 columns of dataset 
## because they are the predictors.
## Note the use of the lapply function

df_norm &lt;- as.data.frame(lapply(df[,c(1:4)], nor))
 
summary(df_norm)

##extract testing set consisting of a 
## subset of 90% of the rows

ran &lt;- sample(1:nrow(df), 0.9 * nrow(df)) 
df_train &lt;- df_norm[ran,] 
df_train_category &lt;- df[ran,5] 


df_test &lt;- df_norm[-ran,]  
df_test_category &lt;- df[-ran,5]


##load the package class
 library(class) 
##run knn function
 pr &lt;- knn(df_train,df_test,cl=df_train_category,k=5)
 
 ##create confusion matrix
CC &lt;- table(pr,df_test_category)
 
## accuracy is the ratio of  correct predictions 
## by total number of predictions 
 
 accuracy &lt;- function(x){
                  sum(diag(x)/(sum(x,na.rm=TRUE))) * 100
		  }
 accuracy(CC)
</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Daniel Ballesteros-Chávez</p>
<p class="date">Created: 2023-01-11 Wed 15:19</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
