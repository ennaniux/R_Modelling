#+title: Lecture 2
#+author: Daniel Ballesteros-Chávez
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.1 (Org mode 9.3.6)
#+PROPERTY: header-args :R+ :exports both
#+PROPERTY: header-args :R+ :session *R*


# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{i=1}^n&space;(x_i&space;-&space;\bar{x})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{i=1}^n&space;(x_i&space;-&space;\bar{x})^2" title="\sum_{i=1}^n (x_i - \bar{x})^2" /></a>
# #+html: <p align="center"> <img src="https://render.githubusercontent.com/render/math?math=x_{1,2} = \frac{-b \pm \sqrt{b^2-4ac}}{2b}"></p>
# #+html: <p align="left"> <img src="https://render.githubusercontent.com/render/math?math= \sin^2(x) + \cos^2(x) =1"></p>
# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\sin^2(x)&space;&plus;&space;\cos^2(x)&space;=1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sin^2(x)&space;&plus;&space;\cos^2(x)&space;=1" title="\sin^2(x) + \cos^2(x) =1" /></a>


* Introduction.

In this lecture we will recall some basic concepts from probability and statistics. These can be grouped as

+ Measures of central tendency :: This is a real value that measures a
  "typical value" for a set of observations (more accurately for a
  probability distribution, see below). The most common measures of
  central tendency are the arithmetic mean, the median, and the mode.
+ Measures of dispersion :: 


# * Random Variables

# A random variable /X/ is a measurable function /X : \Omega \rightarrow E/ from a set of possible outcomes \Omega to a measurable space /E/. 

# The technical axiomatic definition requires \Omega to be a sample space of a probability triple ( Ω , F , P ) (see the measure-theoretic definition). 

# A random variable is often denoted by capital roman letters such as /X, Y, Z, T/.

# The probability that X takes on a value in a measurable set S ⊆ E is written as


# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{\color{DarkBlue}&space;{P(&space;X&space;\in&space;S&space;)=&space;P\left(\{\omega&space;\in\Omega&space;\,|\,&space;X(\omega)&space;\in&space;S&space;\}\right)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{\color{DarkBlue}&space;{P(&space;X&space;\in&space;S&space;)=&space;P\left(\{\omega&space;\in\Omega&space;\,|\,&space;X(\omega)&space;\in&space;S&space;\}\right)}}" title="\Large{\color{DarkBlue} {P( X \in S )= P\left(\{\omega \in\Omega \,|\, X(\omega) \in S \}\right)}}" /></a>


* Arithmetic Mean (AM)

It is also known as (a.k.a) average, or mean.

Given a set of real values, usually representing a set of observations,  (x_{1}, x_{2}, ... ,x_{n}) , the /aritmetic mean/ is defined as

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{{\color{DarkBlue}&space;\bar{x}&space;=&space;\frac{1}{n}\sum_{i=1}^n&space;x_i}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{{\color{DarkBlue}&space;\bar{x}&space;=&space;\frac{1}{n}\sum_{i=1}^n&space;x_i}}" title="\Large{{\color{DarkBlue} \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i}}" /></a>

\begin{equation}
\frac{1}{n}\sum_{i=1}^n \, x_i
\end{equation}

* Geometric Mean (GM)

Given a set of real values, usually representing a set of observations,  (x_{1}, x_{2}, ... ,x_{n}) , the /geometric mean/ is defined as

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{\color{DarkBlue}&space;{\bar{x}_{\mbox{geo}}&space;=&space;\left(\Pi_{i=1}^n&space;x_i\right&space;)^{\frac{1}{n}}}&space;=&space;\sqrt[n]{x_1\cdot&space;x_2&space;\cdots&space;x_n}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{\color{DarkBlue}&space;{\bar{x}_{\mbox{geo}}&space;=&space;\left(\Pi_{i=1}^n&space;x_i\right&space;)^{\frac{1}{n}}}&space;=&space;\sqrt[n]{x_1\cdot&space;x_2&space;\cdots&space;x_n}}" title="\Large{\color{DarkBlue} {\bar{x}_{\mbox{geo}} = \left(\Pi_{i=1}^n x_i\right )^{\frac{1}{n}}} = \sqrt[n]{x_1\cdot x_2 \cdots x_n}}" /></a>

\begin{equation}
\bar{x}_{\mbox{geo}}= \left( \Pi_{i=1}^n x_i \right)^{\frac{1}{n}} = \sqrt[n]{x_1\cdot x_2 \cdots x_n}
\end{equation}


* Harmonic mean (HM)

Given a set of real values, usually representing a set of observations,  (x_{1}, x_{2}, ... ,x_{n}) , the /harmonic mean/ is defined as

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{\color{DarkBlue}{&space;\bar{x}_{\mbox{hm}}=\frac{1}{\frac{1}{n}&space;\sum_{i=1}^{n}\frac{1}{x_{i}}}}&space;=&space;\frac{n}{\frac{1}{x_1}&space;&plus;&space;\frac{1}{x_2}&space;&plus;&space;\cdots&space;&plus;&space;\frac{1}{x_n}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{\color{DarkBlue}{&space;\bar{x}_{\mbox{hm}}=\frac{1}{\frac{1}{n}&space;\sum_{i=1}^{n}\frac{1}{x_{i}}}}&space;=&space;\frac{n}{\frac{1}{x_1}&space;&plus;&space;\frac{1}{x_2}&space;&plus;&space;\cdots&space;&plus;&space;\frac{1}{x_n}}}" title="\Large{\color{DarkBlue}{ \bar{x}_{\mbox{hm}}=\frac{1}{\frac{1}{n} \sum_{i=1}^{n}\frac{1}{x_{i}}}} = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + \cdots + \frac{1}{x_n}}}" /></a>

\begin{equation}
\bar{x}_{\mbox{hm}} = \frac{1}{\frac{1}{n} \sum_{i=1}^{n}\frac{1}{x_{i}}} = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + \cdots + \frac{1}{x_n}}
\end{equation}


* Weighted mean (WM)

Given a set of real values, usually representing a set of observations,  (x_{1}, x_{2}, ... ,x_{n}) with assosiated weigths (w_{1}, w_{2}, ... ,w_{n}), the /weighted mean/ is defined as

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;\overline{x}_{w}=\frac{&space;\sum_{i=1}^n&space;w_i&space;x_i}{\sum_{i=1}^n&space;w_i}&space;=&space;\frac{w_1&space;x_1&space;&plus;&space;w_2&space;x_2&space;&plus;&space;\cdots&space;&plus;&space;w_n&space;x_n}{w_1&space;&plus;&space;w_2&space;&plus;&space;\cdots&space;&plus;&space;w_n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;\overline{x}_{w}=\frac{&space;\sum_{i=1}^n&space;w_i&space;x_i}{\sum_{i=1}^n&space;w_i}&space;=&space;\frac{w_1&space;x_1&space;&plus;&space;w_2&space;x_2&space;&plus;&space;\cdots&space;&plus;&space;w_n&space;x_n}{w_1&space;&plus;&space;w_2&space;&plus;&space;\cdots&space;&plus;&space;w_n}" title="\Large\color{DarkBlue} \overline{x}_{w}=\frac{ \sum_{i=1}^n w_i x_i}{\sum_{i=1}^n w_i} = \frac{w_1 x_1 + w_2 x_2 + \cdots + w_n x_n}{w_1 + w_2 + \cdots + w_n}" /></a>

\begin{equation}
\overline{x}_{w}=\frac{\sum_{i=1}^n w_i x_i}{\sum_{i=1}^n w_i} = \frac{w_1 x_1 + w_2x_2 + \cdots + w_n x_n}{w_1 + w_2 + \cdots + w_n}
\end{equation}


** Geometric Interpretation

A well known inequality concerning arithmetic, geometric, and harmonic means for any set of positive numbers is


    AM ≥ GM ≥ HM 

Geometric proof without words that max (a,b) > root mean square (RMS) or quadratic mean (QM) > arithmetic mean (AM) > geometric mean (GM) > harmonic mean (HM) > min (a,b) of two positive numbers a and b 

# #+html: <a title="Cmglee, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:QM_AM_GM_HM_inequality_visual_proof.svg"><img width="512" alt="QM AM GM HM inequality visual proof" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/QM_AM_GM_HM_inequality_visual_proof.svg/512px-QM_AM_GM_HM_inequality_visual_proof.svg.png"></a>



* Mode

The mode is the most frequently occurring value a in data set.

It is possible to have more than one modal value.


* Median

Middle value separating the greater and lesser halves of a data set 

For example if one wants to get the median from the data set: 0, 20, 9, 1, 5:
1) Sort the data: 0, 1, 5, 9, 20.
2) Identify the element exactly in the middle: 0, 1, *5*, 9, 20.
3) Then the median is 5.

On the other hand if the data set is: 5, 11, 1, 3, 15, 20, then
1) Sort the data: 1, 3, 5, 11, 15, 20.
2) Identify the elements in the middle:  1, 3, *5*, *11*, 15, 20.
3) Then the median is 8, which  is the average between 5 and 11: (5 + 11) /2 = 8.

* Minimum and maximum

The minumum and maximum value in a set of observed values and its comparison, is a first approach for the analysis of the dispersion of the data.
Note also that teh Range of the data is defined as:

Range = max - min.


* Quartiles

The quartiles are cut points which divides the number of data points into four parts, or quarters, of more-or-less equal size.

+ The first quartile (Q1) is defined as the middle number between the smallest number (minimum) and the median of the data set. Then,  25% of the data is below this point.

+ The second quartile (Q2) is the median of a data set; thus 50% of the data lies below this point.

+ The third quartile (Q3) is the middle value between the median and the highest value (maximum) of the data set. Then, 75% of the data lies below this point.

Example: Find the quarttiles for

56, 60, 65, 65, 67, 69, 70, 72, 75, 75, 76, 77, 81, 82, 84, 90, 90, 95, 99

First: Check that the set is increasingly ordered.
Second: Since this data set contains even number of observations, we pick the 10th value: 75 as the median.
Third: split the data into two halves including the median
 56, 60, 65, 65, 67, 69, 70, 72, 75, 75
 75, 76, 77, 81, 82, 84, 90, 90, 95, 99
Fourth: In each half compute the median and we will have: Q1 = 68 and Q3=83

/Note/. There are other methods (algorithms) to compute the quartiles, although when the data sets are big, the difference between the outcomes is small. How does R computes the quartiles when using the function =summary=? 

* Variance

Given a set of real values (x_{1}, x_{2}, ... ,x_{n}), the "population" /variance/ is defined as 

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{\color{DarkBlue}&space;{\sigma_X^{2}={\frac&space;{1}{n}}\sum&space;_{i=1}^{n}\left(x_{i}-{\overline&space;{x}}\right)^{2}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{\color{DarkBlue}&space;{\sigma_X^{2}={\frac&space;{1}{n}}\sum&space;_{i=1}^{n}\left(x_{i}-{\overline&space;{x}}\right)^{2}}}" title="\Large{\color{DarkBlue} {\sigma_X^{2}={\frac {1}{n}}\sum _{i=1}^{n}\left(x_{i}-{\overline {x}}\right)^{2}}}" /></a>

\begin{equation}
\sigma_X^{2}={\frac{1}{n}}\sum_{i=1}^{n}\left(x_{i}-{\overline{x}}\right)^{2}.
\end{equation}


When working with samples (subsets) of a given population, the "sample" variance/ is given as

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{\color{DarkBlue}&space;{s_X^{2}={\frac&space;{1}{n-1}}\sum&space;_{i=1}^{n}\left(x_{i}-{\overline&space;{x}}\right)^{2}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{\color{DarkBlue}&space;{s_X^{2}={\frac&space;{1}{n-1}}\sum&space;_{i=1}^{n}\left(x_{i}-{\overline&space;{x}}\right)^{2}}}" title="\Large{\color{DarkBlue} {s_X^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}\left(x_{i}-{\overline {x}}\right)^{2}}}" /></a>

\begin{equation}
s_X^{2}={\frac{1}{n-1}}\sum_{i=1}^{n}\left(x_{i}-{\overline{x}}\right)^{2}.
\end{equation}


* Probability space

A probability space is a special case of a measurable space. This concepts are beyond the scope of this course, but it is worth trying to understand the definition.

Then probability space is a triple $(\Omega, F, P)$ consisting of:

    + The sample space $\Omega$ — an arbitrary non-empty set,
    + The σ-algebra $F \subset 2^{\Omega}$ (also called σ-field), that is, $F$ is a set of subsets of \Omega , called events, with the following properties:
       + $F$  contains the sample space: $\Omega \in F$.
       + $F$  is closed under complements: if $A \in F$, then also $( \Omega \setminus A ) \in F$.
       + $F$  is closed under countable unions: if $A_i \in F$, for $i = 1 , 2 ,\ldots$, then also $( \cup^{\infty}_{i = 1} A_{i} ) \in F$.
    + The probability measure $P : F \to [ 0 , 1 ]$,  a function on F such that with the following properties:
        + $P$ is countably additive (also called σ-additive): if $A_{i} \cap A_{j} = \emptyset$, for $i \neq j$, then $P(\cup^{\infty}_{i = 1} A_{i}) = \Sigma^{\infty}_{i = 1} P(A_{i})$.
        + The measure of entire sample space is equal to one: $P ( \Omega ) = 1$.

* Random Variables

A random variable $X$ is a variable whose value is a numerical outcome of a random event.
There are two types of random variables:
+ Discrete: The possible values of $X$ are separated and individually distinct, e.g. $X \in \{0, 1, 2, . . .\}$.
+ Continuous: Possible values of $X$ from some continuous set, e.g. $X \in (0, \infty), X \in (-\infty, \infty)$, etc.

Formally, a random variable $X$ is a measurable function $X : \Omega
    \to E$, from a set of possible outcomes $\Omega$ to a measurable
    space $E$, which is usually the Real numbers or a subset of
    it. Then, the probability that $X$ takes on a value in a measurable
    set $S \subseteq E$ is written as 

\begin{equation}
P ⁡ ( X ∈ S ) = P ⁡ ( \{ ω ∈ Ω ∣ X ( ω ) ∈ S    \} )
\end{equation}

Examples:

Discrete random variables: 
+ Outcome of a roll of a 6-sided die, that is $X \in \{1, 2, 3, 4, 5, 6\}$.
+ Number of tails in 3 flips of a coin, that is $X \in \{0, 1, 2, 3\}$.

Continuous random variables: 
+ The lifetime $X$ of a light bulb component with $X \in [0, \infty)$.

* Cumulative distribution function

For any random variable $X$, the cumulative distribution function $F (x)$ is given by
\begin{equation}
F (x) = P(X \leq x).
\end{equation}


The following properties are easy to establish

Properties:
1) $0 \leq F(x) \leq 1$ for all $x$.
2) $F(x)$ is non-decreasing.
3) $\lim_{x\to - \infty} F(x) = 0$.
4) $\lim_{x\to + \infty} F(x) = 1$.


* Probability density function

For a continuous random variable $X$, the probability density function $f(x)$ is the derivative of the cumulative distribution function
\begin{equation}
 f(x) = \frac{dF}{dx}.
\end{equation}

By the Fundamental Theorem of Calculus,

\begin{equation}
F(x) =\int_{-\infty}^{x} f(t) dt.
\end{equation}

#+begin_prop
If $f$ is a probability density function then
1) $f(x) \geq 0$.
2) $\int_{-\infty}^{\infty} f(t) dt = 1$.
#+end_prop

* Probabilities

For any pair of real numbers such that $a \leq b$, the probability that the random variable $X$ lies between $a$ and $b$ is 

\begin{equation}
P(a \leq X \leq b) = P(X \leq b) - P( X \leq a) = F(b) - F(a) =  \int_{a}^{b} f(t) dt
\end{equation}

* Mean and variance revisited.

Population mean or expectation of a continuous random variable X is defined as

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;E[X]&space;=&space;\int_{-\infty}^{\infty}&space;x&space;f(x)dx" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;E[X]&space;=&space;\int_{-\infty}^{\infty}&space;x&space;f(x)dx" title="\Large\color{DarkBlue} E[X] = \int_{-\infty}^{\infty} x f(x)dx" /></a>
\begin{equation}
E[X] = \int_{-\infty}^{\infty} xf(x) dx.
\end{equation}

The variance of the random variable X with mean \mu = E[X], is given by
# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;\mbox{Var}[X]&space;=&space;E[(X-\mu)^2]&space;=&space;\int_{-\infty}^{\infty}(x-\mu)^2f(x)dx" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;\mbox{Var}[X]&space;=&space;E[(X-\mu)^2]&space;=&space;\int_{-\infty}^{\infty}(x-\mu)^2f(x)dx" title="\Large\color{DarkBlue} \mbox{Var}[X] = E[(X-\mu)^2] = \int_{-\infty}^{\infty}(x-\mu)^2f(x)dx" /></a>

\begin{equation}
\mbox{Var}[X] = E[(X-\mu)^2] = \int_{-\infty}^{\infty} (x-\mu)^2 f(x) dx.
\end{equation}


The standard deviation is defined by the value 

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;\mbox{s.d}[X]&space;=&space;\sqrt{\mbox{Var}[X]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;\mbox{s.d}[X]&space;=&space;\sqrt{\mbox{Var}[X]}" title="\Large\color{DarkBlue} \mbox{s.d}[X] = \sqrt{\mbox{Var}[X]}" /></a>
\begin{equation}
\mbox{s.d.}[X] = \sqrt{\mbox{Var}[X]}.
\end{equation}


We say that two *random variables are independent* if the realisation of one does not affect the probability distribution of the other. 

Proposition: Let $a, b, c$ be constant real numbers, and $X$, $Y$ be random variables. Then the following identities hold:

1) If $f$ is the probability density function for $X$, and $g$ is any function from the real numbers to the real number, then $E[g(X)] = \int_{-\infty}^{\infty} g(x) f (x) dx$.
2) $E[aX + bY + c] = aE[X] + b E[Y] + c$.
3) $\mbox{Var}[ aX + b] = a^{2} \mbox{Var}[X]$.
4) If $X$ and $Y$ are two independent variables then $E[XY] = E[X]E[Y]$.
5) If $X$ and $Y$ are two independent variables then $\mbox{Var}[X + Y] = \mbox{Var}[X] + \mbox{Var}[Y]$.
6) $\mbox{Var}[X] = E[X^{2}] - (E[X])^{2}$.




* The normal distribution

For real values $-\infty \leq x \leq \infty$, $-\infty \leq \mu \leq \infty$ and  $0 \leq \sigma \leq \infty$, the normal distribution has probability density function:

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;f(x)&space;=&space;\frac{1}{\sigma\sqrt{2\pi}}e&space;^{-\frac{1}{2}\left(&space;\frac{x&space;-&space;\mu}{\sigma}\right&space;)^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;f(x)&space;=&space;\frac{1}{\sigma\sqrt{2\pi}}e&space;^{-\frac{1}{2}\left(&space;\frac{x&space;-&space;\mu}{\sigma}\right&space;)^2}" title="\Large\color{DarkBlue} f(x) = \frac{1}{\sigma\sqrt{2\pi}}e ^{-\frac{1}{2}\left( \frac{x - \mu}{\sigma}\right )^2}" /></a>


In the special case that \sigma = 1 and \mu = 0, then we write the *standard normal distribution* as

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;f(x)&space;=&space;\frac{1}{\sqrt{2\pi}}e&space;^{-\frac{x^2}{2}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;f(x)&space;=&space;\frac{1}{\sqrt{2\pi}}e&space;^{-\frac{x^2}{2}}" title="\Large\color{DarkBlue} f(x) = \frac{1}{\sqrt{2\pi}}e ^{-\frac{x^2}{2}}" /></a>

#+begin_example R
curve(dnorm(x),-3,3)

curve(pnorm(x),-3,3)
#+end_example


*Theorem*. If $X ∼ N (\mu, \sigma^{2} )$ and $Y = aX + b$ for constants $a, b\in \mathbb{R}$, then $Y$ is also normally distributed and 

\begin{equation}
Y \sim N( a \mu + b, a^{2} \sigma^{2})
\end{equation}

In the particular case that $a = 1/\sigma$ and $b =  -\mu / \sigma$, then we actually have

\begin{equation}
Y \sim N( 0, 1).
\end{equation}

Example

For $X \sim N(2,9)$, find $P( 2 < X < 5)$.

[[https://en.wikipedia.org/wiki/Standard_normal_table][table of values may be found here]]

Solution: $P( 2 < X < 5) =  0.34$.


*Example*

Suppose that $Z ∼ N(0, 1)$. Using the probability density function and the definition, show that $\mbox{Var}[Z] = 1$. 


*Example* 

This example is taken from "The R book" by Michael J. Crawley.
Suppose we have measured the heights of 100 people. The mean height was 170 cm and the standard
deviation was 8 cm. We can ask three sorts of questions about data like these: what is the probability that a
randomly selected individual will be:

+ shorter than a particular height?
+ taller than a particular height?
+ between one specified height and another?

The area under the whole curve is exactly 1; everybody has a height between minus infinity and plus infinity.
True, but not particularly helpful. Suppose we want to know the probability that one of our people, selected
at random from the group, will be less than 160 cm tall. We need to convert this height into a value of z; that
is to say, we need to convert 160 cm into a number of standard deviations from the mean. What do we know
about the standard normal distribution? It has a mean of 0 and a standard deviation of 1. So we can convert
any value $y$, from a distribution with mean $ȳ$ and standard deviation $s$ very simply by calculating

\begin{equation}
z = \frac{y − ȳ}{s}
\end{equation}

So we convert $160$ cm into a number of standard deviations. It is less than the mean height ($170$ cm) so its
value will be negative:

\begin{equation}
z = −1.25.
\end{equation}


Now we need to find the probability of a value of the standard normal taking a value of –1.25 or smaller.
This is the area under the left-hand tail (the integral) of the density function. The function we need for this
is pnorm: we provide it with a value of z (or, more generally, with a quantile) and it provides us with the
probability we want:

#+begin_src R :exports both :colnames yes
pnorm(-1.25)
#+end_src

#+RESULTS:
|                 x |
|-------------------|
| 0.105649773666855 |

So the answer to our first question (the shaded area, top left) is just over 10%


* The Bernoulli distribution

This is the probability distribution  a binary random variable $X$. The response takes one of only two
values: it is 1 with probability p  and is 0 with probability $1 – p$ (probability of success and failure respectively). The density function
is given by:

\begin{equation}
f(x) = p^{x}(1-p)^{1-x}.
\end{equation}

Verify that

+ $E[X] = p$.
+ $\mbox{V}ar[X] = pq$.


* The binomial distribution

The general form of the density of the  binomial distribution is given by

# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;f(k)&space;=&space;\binom{n}{k}p^k&space;(1-p)^{n-k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;f(k)&space;=&space;\binom{n}{k}p^k&space;(1-p)^{n-k}" title="\Large\color{DarkBlue} f(k) = \binom{n}{k}p^k (1-p)^{n-k}" /></a>

\begin{equation}
f(k) =  \binom{n}{k} p^{k} (1 - p  )^k.
\end{equation}


The mean of the binomial distribution is np and the variance is np(1 – p).


#+begin_example R
dbinom(x, size, prob)
#+end_example

for example, in a barplot

#+begin_example R 
barplot(pbinom(0:4, 4, 0.1))
barplot(dbinom(0:4, 4, 0.1))
#+end_example 

Compare this plot with the one obtained for the normal distribution.


Example:

Suppose a biased coin comes up heads with probability 0.3 when tossed. The probability of seeing exactly 4 heads in 6 tosses is 
=f(4) = 0.0595=
