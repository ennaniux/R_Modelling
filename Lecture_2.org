#+title: Lecture 2: Recap of Statistics and Probability
#+author: Daniel Ballesteros-Chávez
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.1 (Org mode 9.3.6)
#+PROPERTY: header-args :R+ :exports both
#+PROPERTY: header-args :R+ :session *R*


# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{i=1}^n&space;(x_i&space;-&space;\bar{x})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{i=1}^n&space;(x_i&space;-&space;\bar{x})^2" title="\sum_{i=1}^n (x_i - \bar{x})^2" /></a>
# #+html: <p align="center"> <img src="https://render.githubusercontent.com/render/math?math=x_{1,2} = \frac{-b \pm \sqrt{b^2-4ac}}{2b}"></p>
# #+html: <p align="left"> <img src="https://render.githubusercontent.com/render/math?math= \sin^2(x) + \cos^2(x) =1"></p>
# #+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\sin^2(x)&space;&plus;&space;\cos^2(x)&space;=1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sin^2(x)&space;&plus;&space;\cos^2(x)&space;=1" title="\sin^2(x) + \cos^2(x) =1" /></a>


* Introduction.

In this lecture we will recall some basic concepts from probability and statistics. These can be grouped as

+ Measures of central tendency.
+ Measures of dispersion.

* Random Variables

A random variable /X/ is a measurable function /X : \Omega \rightarrow E/ from a set of possible outcomes \Omega to a measurable space /E/. 

The technical axiomatic definition requires \Omega to be a sample space of a probability triple ( Ω , F , P ) (see the measure-theoretic definition). 

A random variable is often denoted by capital roman letters such as /X, Y, Z, T/.

The probability that X takes on a value in a measurable set S ⊆ E is written as


#+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{\color{DarkBlue}&space;{P(&space;X&space;\in&space;S&space;)=&space;P\left(\{\omega&space;\in\Omega&space;\,|\,&space;X(\omega)&space;\in&space;S&space;\}\right)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{\color{DarkBlue}&space;{P(&space;X&space;\in&space;S&space;)=&space;P\left(\{\omega&space;\in\Omega&space;\,|\,&space;X(\omega)&space;\in&space;S&space;\}\right)}}" title="\Large{\color{DarkBlue} {P( X \in S )= P\left(\{\omega \in\Omega \,|\, X(\omega) \in S \}\right)}}" /></a>


* Arithmetic Mean

It is also known as (a.k.a) average, or mean.

Given a set of real values, usually representing a set of observations,  (x_{1}, x_{2}, ... ,x_{n}) , the aritmetic mean is defined as

#+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{{\color{DarkBlue}&space;\bar{x}&space;=&space;\frac{1}{n}\sum_{i=1}^n&space;x_i}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{{\color{DarkBlue}&space;\bar{x}&space;=&space;\frac{1}{n}\sum_{i=1}^n&space;x_i}}" title="\Large{{\color{DarkBlue} \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i}}" /></a>

* Mode

The mode is the most frequently occurring value a in data set.

It is possible to have more than one modal value.



* Variance

Given a set of real values (x_{1}, x_{2}, ... ,x_{n}), the /variance/ is defined as 

#+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{\color{DarkBlue}&space;{\sigma_X^{2}={\frac&space;{1}{n}}\sum&space;_{i=1}^{n}\left(x_{i}-{\overline&space;{x}}\right)^{2}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{\color{DarkBlue}&space;{\sigma_X^{2}={\frac&space;{1}{n}}\sum&space;_{i=1}^{n}\left(x_{i}-{\overline&space;{x}}\right)^{2}}}" title="\Large{\color{DarkBlue} {\sigma_X^{2}={\frac {1}{n}}\sum _{i=1}^{n}\left(x_{i}-{\overline {x}}\right)^{2}}}" /></a>


When working with samples (subsets) of a given population, the /unbiased sample variance/ is given as

#+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large{\color{DarkBlue}&space;{s_X^{2}={\frac&space;{1}{n-1}}\sum&space;_{i=1}^{n}\left(x_{i}-{\overline&space;{x}}\right)^{2}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large{\color{DarkBlue}&space;{s_X^{2}={\frac&space;{1}{n-1}}\sum&space;_{i=1}^{n}\left(x_{i}-{\overline&space;{x}}\right)^{2}}}" title="\Large{\color{DarkBlue} {s_X^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}\left(x_{i}-{\overline {x}}\right)^{2}}}" /></a>



Given a *vector* $X^T = (x_1, x_2, \ldots, x_p)$, we want to predict the *real value* $Y$, using the linear model
\[ \hat(Y)  = \hat{\beta}_0 + \sum_{j = 1}^p x_j\hat{\beta}_j,\]
where
+ $\hat{Y}$ is the estimated value of $Y$.

+ $\hat{\beta}_0$ is called the /intercept/.

It is possible to write this equation in terms of the usual inner product in $\mathbb{R}^{p}$, for instance, consider the vectors
$X^T = (x_1,\ldtos,x_p)$ and $\hat{\beta} = (\beta_1,\dots,\beta_p)$, then the inner product is defined as the sum of the product of the same entries of the vectors:
\[ \langle X, \hat{\beta} \rangle = x_1 \beta_1 + x_2 \beta_2 + \cdots \x_p \beta_p. \]
Note that in matrix notation it is equivalent to the expression
\[\hat{Y}= X^T \hat{\beta}, \] 
where this is the product of a $1\times n$ matrix with a $n \times 1$ matrix, resulting into a real number.

** Geometric Interpretation

With all this considerations we are able to write the linear model as 
\begin{equatoin}
 \hat{Y}  = \hat{\beta}_0 + X^T \hat{\beta}.
\label{lm01}
\end{equation}

In this case $(X^T, \hat{Y}) is an affine hyperplane cutting the $Y$-axis at the point $(0,\hat{\beta}_0)$.

We can simplyfy a bit more. Instead of considering vectors in $\mathbb{R}^p$, we can add one more coordinate and work in $\mathbb{R}^{p+1}$. If 
we write $\bar{X}^T = (1, X^T) = (1, x_1, \ldots, x_p)$ and $\hat{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)$, then the linear model can be 
expressed simply as
\begin{equaiton}
 \hat{Y} = \bar{X}^T \hat{\beta}. 
\label{lm02}
\end{equation}

In this case $(X^T, \hat{Y})$ is a hyperplane including the origin.




  - Estimating the Coefficients 
  - Assessing the Accuracy of the Coefficient Estimates 
  - Assessing the Accuracy of the Model
  - Comparison of Linear Regression with K-Nearest neighbours
