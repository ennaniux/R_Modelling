#+title: Lecture 4: Statistical inference
#+author: Daniel Ballesteros-Chávez
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.1 (Org mode 9.3.6)
#+PROPERTY: header-args :R+ :exports both
#+PROPERTY: header-args :R+ :session *R*


* Introduction.

Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.[fn:1]

* Sampling and measurement error models

There are two standard paradigms for inference:

+ Sampling model
+ Measurement error model.


Two most common methods of statistical inference are:

+ Confidence intervals
+ Hypotesis testing
  + Paramtetric
  + Non paramtetric






* Estimators and Sampling Distributions


A *parameters* is any value that describe properties of the whole population.    
A function of the random sample {X_1 , . . . , X_n } which is used to estimate the value of
an unknown parameter is called an *estimator*. One can see estimators also as random variables with its own distribution called the *sampling distribution*, and this shows how an estimator would change if we draw repeatedly new samples


** Sampling distribution of the sample mean

Let us select a random sample {X_{1} , . . . , X_{n} } from a population with mean μ and variance σ^{2}, in other words, let
X_{1} , . . . , X_{n} are independent, identically distributed random variables with E [X_{i} ] = μ and Var [X_{i} ] =σ^{2} .


An estimator for the population mean \mu is given by 


Ŷ = \Sigma_{i =1}^{n} X_{i} / n.


Note that the expected value is 


E[Ŷ] = \mu.


Var[Ŷ]  = \sigma^{2} / n.


The *standard error* is the standard deviation of the estimator, i.e. s.e. =  √Var[Ŷ] = \sigma / √n.


If the X_{i} ~ N(\mu, \sigma^{2}) then Ŷ ~ N(\mu , \sigma^{2} / n).


Even if the X_{i} have some other distribution, for large n, the distribution of Ŷ is
approximately normal. This is known as the *Central Limit Theorem* and it is one of the
most famous facts of probability theory.

* The central limit theorem

The Central Limit Theorem of probability states that the sum of many small inde-
pendent random variables will be a random variable with an approximate normal distribution.


For example, the heights of women in the United States follow an approximate
normal distribution. The Central Limit Theorem applies here because height is
affected by many small additive factors. In contrast, the distribution of heights
of all adults in the United States is not so close to normality. The Central Limit
Theorem does not apply here because there is a single large factor—sex—that
represents much of the total variation.

Example 1. Uniform distribution. 

A. Take five uniformly distributed random numbers between 0 and 10 and work out the
average.

#+begin_src R
mean(runif(5,0,10))
#+end_src

#+RESULTS:
: 3.94939380558208

Typically, of course, the average will be close to 5.

B. Let us do this 10,000 times and look at the distribution of the 10,000
means.

#+begin_example R

#Distribution of the raw data
runif(10000,0,10))

#Distribution of the means
means <- numeric(10000)
for (i in 1:10000){
means[i] <- mean(runif(5,0,10))
}
#+end_example

#+RESULTS:

[fn:1] https://en.wikipedia.org/wiki/Statistical_inference
