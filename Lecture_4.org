#+title: Lecture 4: Statistical inference
#+author: Daniel Ballesteros-Chávez
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.1 (Org mode 9.3.6)
#+PROPERTY: header-args :R+ :exports both
#+PROPERTY: header-args :R+ :session *R*


* Introduction.

Statistical inference is the process of using data analysis to infer
properties of an underlying distribution of probability. Inferential
statistical analysis infers properties of a population, for example by
testing hypotheses and deriving estimates. It is assumed that the
observed data set is sampled from a larger population.

* Sampling and measurement error models

There are two standard paradigms for inference:

+ Sampling model
+ Measurement error model.


Two most common methods of statistical inference are:

+ Confidence intervals
+ Hypotesis testing
  + Paramtetric
  + Non paramtetric



* Estimators and Sampling Distributions

The collection of random variables X_{1}, X_{2}, X_{3}, ..., X_{n} is said to be a random sample of size n
if they are independent and identically distributed (i.i.d.), i.e.,
X_{1} , X_{2}, X_{3}, ..., X_{n} are independent random variables, and
they have the same distribution, i.e,
F_{X_1}(x)=F_{X_{2}}(x)=...=F_{X_{n}}(x), for all x∈R.


A *parameters* is any value that describe properties of the whole population.    
A function of the random sample {X_{1} , . . . , X_{n} } which is used to estimate the value of
an unknown parameter is called an *estimator*. One can see estimators also as random variables with its own distribution called the *sampling distribution*, and this shows how an estimator would change if we draw repeatedly new samples


** Sampling distribution of the sample mean

Let us select a random sample {X_{1} , . . . , X_{n} } from a population with mean μ and variance σ^{2}, in other words, let
X_{1} , . . . , X_{n} are independent, identically distributed random variables with E [X_{i} ] = μ and Var [X_{i} ] =σ^{2} .


An estimator for the population mean \mu is given by 


Ŷ = \Sigma_{i =1}^{n} X_{i} / n.


Note that the expected value is 


E[Ŷ] = \mu.


Var[Ŷ]  = \sigma^{2} / n.


The *standard error* is the standard deviation of the estimator, i.e. s.e. =  √Var[Ŷ] = \sigma / √n.


If the X_{i} ~ N(\mu, \sigma^{2}) then Ŷ ~ N(\mu , \sigma^{2} / n).


Even if the X_{i} have some other distribution, for large n, the distribution of Ŷ is
approximately normal. This is known as the *Central Limit Theorem* and it is one of the
most famous facts of probability theory.

* The central limit theorem

The Central Limit Theorem of probability states that the sum of many
small independent random variables will be a random variable with an
approximate normal distribution.


For example, the heights of women in the United States follow an approximate
normal distribution. The Central Limit Theorem applies here because height is
affected by many small additive factors. In contrast, the distribution of heights
of all adults in the United States is not so close to normality. The Central Limit
Theorem does not apply here because there is a single large factor—sex—that
represents much of the total variation.

Example 1. Uniform distribution. 

A. Take five uniformly distributed random numbers between 0 and 10 and work out the
average.

#+begin_src R
mean(runif(5,0,10))
#+end_src

#+RESULTS:
: 3.94939380558208

Typically, of course, the average will be close to 5.

B. Let us do this 10,000 times and look at the distribution of the 10,000
means.

#+begin_example R

#Distribution of the raw data
runif(10000,0,10))

#Distribution of the means
means <- numeric(10000)
for (i in 1:10000){
means[i] <- mean(runif(5,0,10))
}

hist(means)
#+end_example

Now let's take a look at the normal distribution

#+begin_example R
mm  <- mean(means)
ssdd <-  sd(means)
xx  <-  seq(0,10,0.1)
yy  <-  dnorm(xx, mm, ssdd)

plot(xx,yy)
#+end_example

The height that gives the right scale, depends on the chosen bin widths, note that if we doubled with width of the bin there would be (close to) twice
as many numbers in the bin and the bar would be in principle twice as high too. Then get the height of the bars
on our frequency scale we multiply the total frequency, 10 000 by the bin width, 0.5 to get 5000.

#+begin_example R
hist(means)
lines(xx,yy*5000)
#+end_example

* Bias of an estimator

If Ŷ is an estimator of a parameter Y, then Ŷ is called *unbiased* if E[Ŷ] = Y. In general, the value
E[Ŷ] - Y is called the bias of the estimator and if it is different from zero, the estimator is said to be *biased*.


Example. For a random sample {X_{1} , . . . , X_{n} } from a population with E [X_{i} ] = μ and Var [X_{i} ] =σ^{2} .

Se have seen that the estimator

Ŷ = \Sigma_{i =1}^{n} X_{i} / n.

Is an unbiased estimator for the mean \mu.

Show that the sample variance

ŝ^2 = 1/(n-1) \Sigma_{i=1}^{n} (X_{i} - Ŷ)^{2},

is an unbiased estimator of \sigma^{2}.

On the other hand, the sample standard deviation ŝ, is biased.
to see this, note that ŝ is random, so Var(ŝ)>0, and 
Var(ŝ) = E[ŝ^{2}] - E[ŝ]^{2}.


* Maximum Likelihood Estimation

Apart from the mean and variance estimators, it is not clear how we can estimate other parameters. 
We now would like to talk about a systematic way of parameter estimation. 
Specifically, we would like to introduce an estimation method, called maximum likelihood estimation (MLE). 

Example

I have a bag that contains 3 balls. Each ball is either red or blue,
but I have no information in addition to this. Thus, the number of
blue balls, call it θ, might be 0, 1, 2, or 3. I am allowed to choose
4 balls at random from the bag with replacement. We define the random
variables X_{1}, X_{2}, X_{3}, and X_{4}
as follows
X_{i}= 1 if the i-th chosen ball is blue, and X_{i} = 0 if it is red.
Note that X{i}'s are i.i.d. and X_{i}∼Bernoulli(\theta / 3). Lets work out the most likely value for
\theta.


If after the experiment I obtain the values X_{1} = 1, X_{2} = 0, X_{3}=1, X_{4} = 1, i.e., three blues and 2 reds, then 
+ For each possible value of \theta, find the probability of the observed sample.
+ For which value of \theta is the probability of the sample the largest?


 Let X_{1}, X_{2}, X_{3}, ..., X_{n} be a random sample from a distribution with a
 parameter θ (In general, θ might be a vector, θ=(θ_{1},θ_{2},⋯,θ_{k}).)
 Suppose that x_{1}, x_{2}, x_{3}, ..., x_{n} are the observed values of X_{1}, X_{2},
, X_{n}. We define the
 likelihood function as the probability of the observed sample as a
 function of θ:

L(x_{1},...,x_{n} ; \theta ) = P(X_{1} = x_{1}, X_{2} = x_{2}, ..., X_{n} = x_{n};\theta )

A maximum likelihood estimate of θ, is a value of θ that maximises the likelihood function.

A maximum likelihood estimator (MLE) of the parameter θ is a random variable whose value when
X_{i} = x_{i}, equals the maximum likelihood estimate of \theta.


Examples:

For X_{i} ~ Binom(3,\theta), and observed values (x_{1}, x_{2}, x_{3},
x_{4}) = (1, 3, 2, 2), find the likelihood function, find the maximum
likelihood estimate of θ


For X_{i} ~ Exponential(\theta) and observed values x_{1}, x_{2}, ..., x_{n}, find the maximum likelihood estimator (MLE) of θ


Generalised to several variables.

Example 

Suppose that we have observed the random sample X_{1}, X_{2}, X_{3},
..., X_{n}, where X_{i} ∼ N(θ_{1},θ_{2}). Find the maximum likelihood
estimators for θ_{1} and θ_{2}.



* Confidence intervals

Let X_{1}, X_{2}, ..., X_{n} be a random sample from a distribution
with a parameter θ that is to be estimated. An interval estimator with
confidence level 1−α consists of two estimators such that

#+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;P\left(\hat{\Theta}_l&space;\leq&space;\theta&space;\leq&space;\hat{\Theta}_h&space;\right&space;)&space;\geq&space;1&space;-&space;\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;P\left(\hat{\Theta}_l&space;\leq&space;\theta&space;\leq&space;\hat{\Theta}_h&space;\right&space;)&space;\geq&space;1&space;-&space;\alpha" title="\Large\color{DarkBlue} P\left(\hat{\Theta}_l \leq \theta \leq \hat{\Theta}_h \right ) \geq 1 - \alpha" /></a>

Equivalently, we say that this is a (1-\alpha) 100% confice interval for \theta.



Example

Let Z∼N(0,1), find x_{l} and x_{h} such that

P(x_{l}≤ Z ≤x_{h})=0.95


Solution: x_{l} = -1.96, and x_{h}  = 1.96


Example

Given a random sample $X_1, X_2, \ldots, X_n$ from a standard normal distribution. Find the confidence interval for the parameter $\theta$ with a confidence level of 95%.

#+html: <a href="https://www.codecogs.com/eqnedit.php?latex=\Large\color{DarkBlue}&space;\left[&space;\overline{X}&space;-&space;\frac{1.96}{\sqrt{n}}&space;,&space;\overline{X}&space;&plus;&space;\frac{1.96}{\sqrt{n}}&space;\right]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Large\color{DarkBlue}&space;\left[&space;\overline{X}&space;-&space;\frac{1.96}{\sqrt{n}}&space;,&space;\overline{X}&space;&plus;&space;\frac{1.96}{\sqrt{n}}&space;\right]" title="\Large\color{DarkBlue} \left[ \overline{X} - \frac{1.96}{\sqrt{n}} , \overline{X} + \frac{1.96}{\sqrt{n}} \right]" /></a>
