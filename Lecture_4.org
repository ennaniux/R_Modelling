#+title: Lecture 4: Statistical inference
#+author: Daniel Ballesteros-Chávez
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.1 (Org mode 9.3.6)
#+PROPERTY: header-args :R+ :exports both
#+PROPERTY: header-args :R+ :session *R*


* Introduction.

Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.[fn:1]

* Sampling and measurement error models

There are two standard paradigms for inference:

+ Sampling model
+ Measurement error model.


Two most common methods of statistical inference are:

+ Confidence intervals
+ Hypotesis testing
  + Paramtetric
  + Non paramtetric






* Estimators and Sampling Distributions

The collection of random variables X_{1}, X_{2}, X_{3}, ..., X_{n} is said to be a random sample of size n
if they are independent and identically distributed (i.i.d.), i.e.,
X1 , X2, X3, ..., Xn are independent random variables, and
they have the same distribution, i.e,
F_{X1}(x)=F_{X2}(x)=...=F_{Xn}(x), for all x∈R.


A *parameters* is any value that describe properties of the whole population.    
A function of the random sample {X_1 , . . . , X_n } which is used to estimate the value of
an unknown parameter is called an *estimator*. One can see estimators also as random variables with its own distribution called the *sampling distribution*, and this shows how an estimator would change if we draw repeatedly new samples


** Sampling distribution of the sample mean

Let us select a random sample {X_{1} , . . . , X_{n} } from a population with mean μ and variance σ^{2}, in other words, let
X_{1} , . . . , X_{n} are independent, identically distributed random variables with E [X_{i} ] = μ and Var [X_{i} ] =σ^{2} .


An estimator for the population mean \mu is given by 


Ŷ = \Sigma_{i =1}^{n} X_{i} / n.


Note that the expected value is 


E[Ŷ] = \mu.


Var[Ŷ]  = \sigma^{2} / n.


The *standard error* is the standard deviation of the estimator, i.e. s.e. =  √Var[Ŷ] = \sigma / √n.


If the X_{i} ~ N(\mu, \sigma^{2}) then Ŷ ~ N(\mu , \sigma^{2} / n).


Even if the X_{i} have some other distribution, for large n, the distribution of Ŷ is
approximately normal. This is known as the *Central Limit Theorem* and it is one of the
most famous facts of probability theory.

* The central limit theorem

The Central Limit Theorem of probability states that the sum of many small inde-
pendent random variables will be a random variable with an approximate normal distribution.


For example, the heights of women in the United States follow an approximate
normal distribution. The Central Limit Theorem applies here because height is
affected by many small additive factors. In contrast, the distribution of heights
of all adults in the United States is not so close to normality. The Central Limit
Theorem does not apply here because there is a single large factor—sex—that
represents much of the total variation.

Example 1. Uniform distribution. 

A. Take five uniformly distributed random numbers between 0 and 10 and work out the
average.

#+begin_src R
mean(runif(5,0,10))
#+end_src

#+RESULTS:
: 3.94939380558208

Typically, of course, the average will be close to 5.

B. Let us do this 10,000 times and look at the distribution of the 10,000
means.

#+begin_example R

#Distribution of the raw data
runif(10000,0,10))

#Distribution of the means
means <- numeric(10000)
for (i in 1:10000){
means[i] <- mean(runif(5,0,10))
}

hist(means)
#+end_example

Now let's take a look at the normal distribution

#+begin_example R
mm  <- mean(means)
ssdd <-  sd(means)
xx  <-  seq(0,10,0.1)
yy  <-  dnorm(xx, mm, ssdd)

plot(xx,yy)
#+end_example

The height that gives the right scale, depends on the chosen bin widths, note that if we doubled with width of the bin there would be (close to) twice
as many numbers in the bin and the bar would be in principle twice as high too. Then get the height of the bars
on our frequency scale we multiply the total frequency, 10 000 by the bin width, 0.5 to get 5000.

#+begin_example R
hist(means)
lines(xx,yy*5000)
#+end_example

* Bias of an estimator

If Ŷ is an estimator of a parameter Y, then Ŷ is called *unbiased* if E[Ŷ] = Y. In general, the value
E[Ŷ] - Y is called the bias of the estimator and if it is different from zero, the estimator is said to be *biased*.


Example. For a random sample {X_{1} , . . . , X_{n} } from a population with E [X_{i} ] = μ and Var [X_{i} ] =σ^{2} .

Se have seen that the estimator

Ŷ = \Sigma_{i =1}^{n} X_{i} / n.

Is an unbiased estimator for the mean \mu.

Show that the sample variance

ŝ^2 = 1/(n-1) \Sigma_{i=1}^{n} (X_{i} - Ŷ)^2,

is an unbiased estimator of \sigma^2.

On the other hand, the sample standard deviation ŝ, is biased.
to see this, note that ŝ is random, so Var(ŝ)>0, and 
Var(ŝ) = E[ŝ^2] - E[ŝ]^2.


* Maximum Likelihood Estimation

Appart from the mean and variance estimators, it is not clear how we can estimate other parameters. 
We now would like to talk about a systematic way of parameter estimation. 
Specifically, we would like to introduce an estimation method, called maximum likelihood estimation (MLE). 

Example

I have a bag that contains 3 balls. Each ball is either red or blue,
but I have no information in addition to this. Thus, the number of
blue balls, call it θ, might be 0, 1, 2, or 3. I am allowed to choose
4 balls at random from the bag with replacement. We define the random
variables X_{1}, X_{2}, X_{3}, and X_{4}
as follows
Xi= 1 if the ith chosen ball is blue, and X_{i} = 0 if it is red.
Note that Xi's are i.i.d. and Xi∼Bernoulli(\Theta / 3). Lets work out the most likely value for
\Theta.

[fn:1] https://en.wikipedia.org/wiki/Statistical_inference
