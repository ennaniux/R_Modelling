<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-11-29 Tue 09:39 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Linear regression</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Daniel Ballesteros-Chávez" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style type="text/css"> tr:nth-child(odd) {background-color: #e2e2e2;}  tr:first-child {font-weight: bold}  tr:hover {background-color: #d0c6e5;}</style>
<style>
/* Daniel Ballesteros-Chavez */
/* DBCh CSS for blog project */
/* color schemes: #333745; #E63462 ; #C7EFCF ; #EEF5DB ; #909396; #262626;*/
/* Modified version with responsive TOC
/* usage: #+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./style01.css"/> */
body {
font-size: 18px;
color: #404040;
/* background-color: #333745; */
font-family: Helvetica;
line-height: 1.3;
}
#content {
max-width: 50em;
margin-left: auto;
margin-right: auto;
padding: 15px 50px 50px 15px;
background-color: #E4F7FF;
}
p {
text-align: justify;
}
/* this part is about the table of contents TOC */
#table-of-contents a:link,
#table-of-contents a:visited {
color: #404040;
background: transparent;
}
#table-of-contents a:hover {
background-color: #ccc;
color: #404040;
}
#table-of-contents {
line-height: 1.2;
}
#table-of-contents h2 {
background-color:  #ccc ;
padding-left: 0.3em;
color: #404040;
border-bottom: 0;
}
#table-of-contents ul {
list-style: none;
padding-left: 0.3em;
font-weight: normal;
}
#table-of-contents div>ul>li {
margin-top: 1em;
font-weight: bold;
}
#table-of-contents .tag {
display: none;
}
#table-of-contents .todo,
#table-of-contents .done {
font-size: 80%;
}
#table-of-contents ol>li {
margin-top: 1em;
}
@media screen {
#table-of-contents {
position: fixed;
top: 0;
left: 0;
padding: 1em 0 1em 1em;
width: 290px;
height: 100vh;
overlow-x: hidden;
overlow-y: auto;
overflow: auto;
}
#table-of-contents h2 {
margin-top: 0;
font-family: Helvetica,Arial,"Lucida Grande",sans-serif;
}
#table-of-contents code {
font-size: 12px;
}
}
@media screen and (max-width: 95em) {
#table-of-contents {
display: none;
}
h1.title {
margin-left: 0%;
}
div#content {
margin-left: 5%;
max-width: 90%;
}
}
/*Html Boxes around THMs and Propositions */
.abstract  {
color:  #404040;
border: 1px solid #404040;
box-shadow: 3px 3px 3px ;
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.abstract:before {
display: inline;
position: absolute;
background-color: white;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Abstract';
}
.mydef  {
color:  #404040;
border: 1px solid #404040;
background-color: #FFD580;
/* box-shadow: 3px 3px 3px orange; */
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.mydef:before {
display: inline;
position: absolute;
/* background-color: white; */
background-color: orange;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Definition';
}
.prop  {
color:  #404040;
border: 1px solid ;
background-color: #F1FFC2;
/* box-shadow: 3px 3px 3px green; */
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.prop:before {
color:  white;
display: inline;
position: absolute;
background-color: green;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Proposition';
}
.thm  {
color:  #404040;
border: 1px solid ;
background-color: lightcyan;
/* box-shadow: 3px 3px 3px brown; */
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.thm:before {
color:  white;
display: inline;
position: absolute;
background-color: darkblue;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Theorem';
}
.cor  {
color:  #404040;
border: 1px solid blue;
box-shadow: 3px 3px 3px blue;
padding: 8pt;
overflow: auto;
margin: 1.2em;
position: relative;
overflow: auto;
padding-top: 1.2em;
}
.cor:before {
display: inline;
position: absolute;
background-color: white;
top: -5px;
left: 10px;
padding: 3px;
border: 1px solid black;
content: 'Corollary';
}
/*defaults form org-mode export */
.title  { text-align: center; }
.todo   { font-family: monospace; color: red; }
.done   { color: green; }
.tag    { background-color: #eee; font-family: monospace;
padding: 2px; font-size: 80%; font-weight: normal; }
.timestamp { color: #bebebe; }
.timestamp-kwd { color: #5f9ea0; }
.right  { margin-left: auto; margin-right: 0px;  text-align: right; }
.left   { margin-left: 0px;  margin-right: auto; text-align: left; }
.center { margin-left: auto; margin-right: auto; text-align: center; }
.underline { text-decoration: underline; }
#postamble p, #preamble p { font-size: 90%; margin: .2em; text-align: center;}
p.verse { margin-left: 3%; }
pre {
border: 1px solid #ccc;
box-shadow: 3px 3px 3px #eee;
padding: 8pt;
font-family: monospace;
overflow: auto;
margin: 1.2em;
}
pre.src {
position: relative;
overflow: auto;
padding-top: 1.2em;
}
pre.src:before {
display: none;
position: absolute;
background-color: white;
top: -10px;
right: 10px;
padding: 3px;
border: 1px solid black;
}
pre.src:hover:before { display: inline;}
pre.src-sh:before    { content: 'sh'; }
pre.src-bash:before  { content: 'sh'; }
pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
pre.src-R:before     { content: 'R'; }
pre.src-perl:before  { content: 'Perl'; }
pre.src-java:before  { content: 'Java'; }
pre.src-sql:before   { content: 'SQL'; }
table { border-collapse:collapse; }
caption.t-above { caption-side: top; }
caption.t-bottom { caption-side: bottom; }
td, th { vertical-align:top;  }
th.right  { text-align: center;  }
th.left   { text-align: center;   }
th.center { text-align: center; }
td.right  { text-align: right;  }
td.left   { text-align: left;   }
td.center { text-align: center; }
dt { font-weight: bold; }
.footpara:nth-child(2) { display: inline; }
.footpara { display: block; }
.footdef  { margin-bottom: 1em; }
.figure { padding: 1em; }
.figure p { text-align: center; }
.inlinetask {
padding: 10px;
border: 2px solid gray;
margin: 10px;
background: #ffffcc;
}
#org-div-home-and-up
{ text-align: right; font-size: 70%; white-space: nowrap; }
textarea { overflow-x: auto; }
.linenr { font-size: smaller }
.code-highlighted { background-color: #ffff00; }
.org-info-js_info-navigation { border-style: none; }
#org-info-js_console-label
{ font-size: 10px; font-weight: bold; white-space: nowrap; }
.org-info-js_search-highlight
{ background-color: #ffff00; color: #000000; font-weight: bold; }
</style>
<style>code {background-color: #ccc}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Linear regression</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org1a27fec">1. Introduciton</a></li>
<li><a href="#org88d72c1">2. Estimating the Coefficients</a></li>
<li><a href="#org9cf9cae">3. Least squares method</a></li>
<li><a href="#orgc18045b">4. Covariance</a>
<ul>
<li><a href="#org9b56af4">4.1. Definition</a></li>
</ul>
</li>
<li><a href="#orgc5773be">5. Second method to obtain the coefficients</a></li>
<li><a href="#orgda9b012">6. Standard errors</a></li>
<li><a href="#org691feb2">7. Assessing the Accuracy of the Model</a></li>
<li><a href="#orgdc98ad8">8. Example</a>
<ul>
<li><a href="#orga36048b">8.1. </a></li>
</ul>
</li>
<li><a href="#org0beb226">9. \(R^2\) - statistic</a></li>
<li><a href="#orgc77e5a1">10. \(F\) - statistic</a></li>
<li><a href="#orgd3633f3">11. Multiple linear regressions.</a></li>
<li><a href="#org9412e82">12. Estimating the Regression Coefficients</a></li>
<li><a href="#org36d7488">13. Important questions</a></li>
<li><a href="#org127484c">14. Is at least one of the predictors useful?</a></li>
<li><a href="#orgdaf5e2c">15. Deciding on Important Variables</a></li>
<li><a href="#orga7ebf54">16. Deciding on Important Variables</a></li>
<li><a href="#org89f0fd7">17. Deciding on Important Variables</a>
<ul>
<li><a href="#org17a13be">17.1. Forward selection</a></li>
</ul>
</li>
<li><a href="#org1c74709">18. </a>
<ul>
<li><a href="#org016b630">18.1. Backward selection.</a></li>
<li><a href="#orgeb8c02e">18.2. Mixed selection</a></li>
</ul>
</li>
<li><a href="#org4be2c22">19. Model Fit</a></li>
<li><a href="#org602097a">20. Predictions</a></li>
<li><a href="#orgd40a3f2">21. Example</a></li>
<li><a href="#org23c8c99">22. References</a></li>
</ul>
</div>
</div>


<div id="outline-container-org1a27fec" class="outline-2">
<h2 id="org1a27fec"><span class="section-number-2">1</span> Introduciton</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>The idea behind the linear regression model is that we would like to
predict a dependent variable \(Y\) using the independent variable \(X\)
assuming that they are related in a linear way.</li>
</ul>


<ul class="org-ul">
<li>This is a very simple assumption, nevertheless, linear regression is
still a useful and widely used statistical method.</li>

<li>Many fancy statistical learning approaches can be seen as
generalizations or extensions of linear regression. Consequently,
the importance of having a good understanding of linear regression
before studying more complex learning methods cannot be overstated.</li>
</ul>



<p>
This is a very basic
approach for predicting a quantitative response \(Y\) on the basis of a sin-
gle predictor variable \(X\). It assumes that there is approximately a linear
relationship between \(X\) and \(Y\).
</p>

\begin{equation}
Y  \approx  \alpha + \beta X
\end{equation}


<p>
Where the constants \(\alpha\) and \(\beta\) will be determined.
</p>
<ul class="org-ul">
<li>\(\alpha\) is called the intercept.</li>
<li>\(\beta\) is called the slope.</li>
</ul>


<p>
Hence, we would like to estimate the values of \(Y\) on the basis of \(X = x\). We will denote
this estimated value by \(\hat{y}\). Usually the intercept and slope are also unknown, so they are to
be estimated. In this case we write
</p>
\begin{equation}
\hat{y} = \hat{\alpha} + \hat{\beta}x.
\end{equation}
</div>
</div>


<div id="outline-container-org88d72c1" class="outline-2">
<h2 id="org88d72c1"><span class="section-number-2">2</span> Estimating the Coefficients</h2>
<div class="outline-text-2" id="text-2">
<p>
In order to estimate the coefficitents \(\alpha\) and \(\beta\), we will
use the data we have at our disposal, for instance \(\{ x_1, x_2,
\ldots, x_n\}\) and \(\{ y_1, y_2, \ldots, y_n\}\).
</p>


<p>
Then we want to find the coefficient esitmates \(\hat{\alpha}\) and \(\hat{\beta}\) such that for
\(i = 1, 2, \ldots, n\) we have
</p>
\begin{equation}
y_i \approx \hat{\alpha} + \hat{\beta}x_i.
\end{equation}

<p>
We want to find the straight line determined by \(\hat{\alpha}\) and \(\hat{\beta}\) 
that is as closed as possible to the \(n\) pairs of points \((x_i, y_i) \in \mathbb{R}^2\), \(i =1, 2, 
\ldots,n\).
</p>



<p>
For this we need a metric to decide what do we mean by <i>to be close to the points</i>.
</p>

<p>
Define the \(i\) -th residual as the difference between the \(i\) -th observed response (value of Y) and 
the \(i\) -th predicted value by our estimation:
</p>
\begin{equation}
\begin{split}
\epsilon_i &= y_i - \hat{y}_i \\
&= y_i - \hat{\alpha} - \hat{\beta}x_i \\
\end{split}
\end{equation}



<p>
Now we define the function \(g:\mathbb{R}^2 \to \mathbb{R}\) as
</p>

\begin{equation}
\begin{split}
g(\alpha, \beta)& = \sum_{i=1}^n \epsilon_i^2\\
&=\sum_{i=1}^{n}\left( y_i - \hat{\alpha} - \hat{\beta}x_i \right)^2.\\
\end{split}
\end{equation}

<p>
This function \(g\) is usually called  residual sum of squares (RSS).
</p>
</div>
</div>


<div id="outline-container-org9cf9cae" class="outline-2">
<h2 id="org9cf9cae"><span class="section-number-2">3</span> Least squares method</h2>
<div class="outline-text-2" id="text-3">
<p>
By solving \(\partial g/\partial \alpha = 0\) and \(\partial g/ \partial \beta = 0\) one gets the cirtical point
</p>

\begin{equation}
\begin{split}
\hat{\beta} & = \frac{ \sum_{i=1}^{n} \left(x_i - \bar{x}\right)(y_i - \bar{y}) }{\sum_{i=1}^{n}( x_i - \bar{x})^2},\\
\hat{\alpha}& = \bar{y} - \hat{\beta}\bar{x},
\end{split}
\end{equation}

<p>
where
</p>

\begin{equation}
\bar{x}  = \frac{1}{n}\sum_{i=1}^{n} x_i, \quad \mbox{and}\quad \bar{y}  = \frac{1}{n}\sum_{i=1}^{n} y_i
\end{equation}
</div>
</div>

<div id="outline-container-orgc18045b" class="outline-2">
<h2 id="orgc18045b"><span class="section-number-2">4</span> Covariance</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org9b56af4" class="outline-3">
<h3 id="org9b56af4"><span class="section-number-3">4.1</span> Definition</h3>
<div class="outline-text-3" id="text-4-1">
<p>
For two jointly distributed real-valued random variables \(X\) and \(Y\) ( both with finite variance), the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values:
</p>
\begin{equation}
\displaystyle \operatorname {cov} (X,Y)=\operatorname {E} {{\big [}(X-\operatorname {E} [X])(Y-\operatorname {E} [Y]){\big ]}} 
\end{equation}

<p>
If the random variables are descrete, this can also be writte as
</p>

\begin{equation}
\displaystyle \operatorname {cov} (X,Y)=\frac{1}{n}\sum_{i=1}^{n} {{\big [}(X_i-\overline{X})(Y_{i}-\overline{Y}){\big ]}} 
\end{equation}
</div>
</div>
</div>


<div id="outline-container-orgc5773be" class="outline-2">
<h2 id="orgc5773be"><span class="section-number-2">5</span> Second method to obtain the coefficients</h2>
<div class="outline-text-2" id="text-5">
<p>
By writing \(\overline{X} = E[X]\), and the last definition then it is possible to write
</p>

\begin{equation}
\begin{split}
\hat{\beta}  &= \frac{\operatorname{cov} (X,Y)}{\operatorname{var}(X)},\\
\hat{\alpha} &= \operatorname{E}[Y] - \beta\operatorname{E}[X] .
\end{split}
\end{equation}
</div>
</div>

<div id="outline-container-orgda9b012" class="outline-2">
<h2 id="orgda9b012"><span class="section-number-2">6</span> Standard errors</h2>
<div class="outline-text-2" id="text-6">
<p>
Recall that by <i>standard error</i> (deviation), we mean the  square root of the variance of an estimator,
in case of our estimate for the coefficients:
</p>

\begin{equation}
\begin{split}
\operatorname{SE}^2(\hat{\alpha}) &= \sigma^2\left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \overline{x})^2}\right], \\
\operatorname{SE}^2(\hat{\beta}) &= \frac{\sigma^2}{\sum_{i=1}^n (x_i - \overline{x})^2}.
\end{split}
\end{equation}

<p>
Where  \(\sigma^2 = \mbox{Var}(\epsilon)\). In general, \(\sigma\) is not known, but can be estimated from the data. 
This estimate is known as the <i>residual standard error</i>, and is given by the formula
</p>

\begin{equation}
\mbox{RSE} = \sqrt{\frac{\mbox{RSS}}{n-2}}
\end{equation}

<p>
Roughly speaking, it is the average amount that the response
will deviate from the true regression line.
</p>
</div>
</div>

<div id="outline-container-org691feb2" class="outline-2">
<h2 id="org691feb2"><span class="section-number-2">7</span> Assessing the Accuracy of the Model</h2>
<div class="outline-text-2" id="text-7">
<p>
Standard errors can also be used to perform hypothesis tests on the
coefficients. The most common hypothesis test involves testing the null
hypothesis of
</p>

<p>
\(H_0\) : There is no relationship between \(X\) and \(Y\).
</p>

<p>
\(H_1\) : There is a relationship between \(X\) and \(Y\).
</p>

<p>
Equivalently:
</p>

\begin{equation}
\begin{split}
H_0 : \beta = 0,\\
H_1 : \beta \neq 0.\\
\end{split}
\end{equation}


<p>
This usually uses the \(t\) -statistic
</p>
\begin{equation}
t = \frac{\hat{\beta} - 0}{\mbox{SE}(\hat{\beta})}.
\end{equation}

<p>
If there really is no relationship between X and Y , then we expect
that \(t\) will have a t-distribution with n − 2 degrees of freedom.
</p>

<p>
<i>We reject the null hypothesis</i> — that is, we declare a relationship
to exist between X and Y — if the p-value is small enough.
</p>
</div>
</div>

<div id="outline-container-orgdc98ad8" class="outline-2">
<h2 id="orgdc98ad8"><span class="section-number-2">8</span> Example</h2>
<div class="outline-text-2" id="text-8">
<p>
The linear model is implemented in R via the function <code>lm</code>.
</p>

<p>
From the data set iris, contrast the columns for <i>Sepal length</i> and 
<i>Petal length</i> and fit a linear model:
</p>
</div>
<div id="outline-container-orga36048b" class="outline-3">
<h3 id="orga36048b"><span class="section-number-3">8.1</span> </h3>
<div class="outline-text-3" id="text-8-1">
<p>
\tiny
</p>
<pre class="example">
df  &lt;-  iris
model01  &lt;-  lm(df$Sepal.Length ~ df$Petal.Length)
summary(model01)

Call:
lm(formula = df$Sepal.Length ~ df$Petal.Length)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.24675 -0.29657 -0.01515  0.27676  1.00269 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      4.30660    0.07839   54.94   &lt;2e-16 ***
df$Petal.Length  0.40892    0.01889   21.65   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4071 on 148 degrees of freedom
Multiple R-squared:   0.76,	Adjusted R-squared:  0.7583 
F-statistic: 468.6 on 1 and 148 DF,  p-value: &lt; 2.2e-16

</pre>
</div>
</div>
</div>

<div id="outline-container-org0beb226" class="outline-2">
<h2 id="org0beb226"><span class="section-number-2">9</span> \(R^2\) - statistic</h2>
<div class="outline-text-2" id="text-9">
<p>
The \(R^2\) statistic is a measure of the linear (sample) relationship between \(X\) and
\(Y\) namely \(\mbox{cor}(X,Y)\). It is also a measure of the linear relationship between X and Y .
</p>

\begin{equation}
R^2 = \frac{\mbox{TSS} - \mbox{RSS}}{\mbox{TSS}}  = 1 - \frac{\mbox{RSS}}{\mbox{TSS}}
\end{equation}

<p>
Here
</p>

\begin{equation}
\mbox{TSS} = \sum_{i=1}^{n} (y_i - \overline{y})^2.
\end{equation}


<p>
An \(R^2\) statistic that is
close to 1 indicates that a large proportion of the variability in the response
has been explained by the regression. A number near 0 indicates that the
regression did not explain much of the variability in the response; this might
occur because the linear model is wrong, or the inherent error \(\sigma^2\) is high,
or both.
</p>
</div>
</div>


<div id="outline-container-orgc77e5a1" class="outline-2">
<h2 id="orgc77e5a1"><span class="section-number-2">10</span> \(F\) - statistic</h2>
<div class="outline-text-2" id="text-10">
<p>
This is used in multiple linear regression when we want to fit 
</p>
\begin{equation}
Y = \alpha + \sum_{j=1}^{p} \beta_{j} X_j + \epsilon.
\end{equation}

<p>
It is defined by
</p>
\begin{equation}
F = \frac{\mbox{TSS} - \mbox{RSS}}{\mbox{RSS}} \cdot \frac{ n-p-1}{p}.
\end{equation}

<p>
Hence, when there is no relationship between the response and predictors,
one would expect the F-statistic to take on a value close to 1. And on the other hand
if \(F\) is greater than \(1\) then the hypothesis that at least one of the \(\beta_i\)'s is
different from zero have strong evidence.
</p>
</div>
</div>


<div id="outline-container-orgd3633f3" class="outline-2">
<h2 id="orgd3633f3"><span class="section-number-2">11</span> Multiple linear regressions.</h2>
<div class="outline-text-2" id="text-11">
<p>
Simple linear regression is a useful approach for predicting a response on the
basis of a single predictor variable. However, in practice we often have more
than one predictor.
</p>

<p>
One option is to run three separate simple linear regressions, each of
which uses a different independent variable as a predictor.
</p>

<p>
Instead of fitting a separate simple linear regression model for each predictor,
 a better approach is to extend the simple linear regression model
so that it can directly accommodate multiple predictors.
</p>

<p>
This is used in multiple linear regression when we want to fit 
</p>
\begin{equation}
Y = \alpha + \sum_{j=1}^{p} \beta_{j} X_j + \epsilon.
\end{equation}
</div>
</div>



<div id="outline-container-org9412e82" class="outline-2">
<h2 id="org9412e82"><span class="section-number-2">12</span> Estimating the Regression Coefficients</h2>
<div class="outline-text-2" id="text-12">
<p>
As in the case of Simple linear regression, we look to minimise the residual sum squared:
</p>

\begin{equation}
\mbox{RSS} = \sum_{i=1}^{n} (y_i - \alpha - \beta_1 x_{i1} - \beta_2 x_{i2} - \cdots \beta_p x_{ip} ) ^2.
\end{equation}


<p>
And we proceed as in the  simple linear case.
</p>
</div>
</div>

<div id="outline-container-org36d7488" class="outline-2">
<h2 id="org36d7488"><span class="section-number-2">13</span> Important questions</h2>
<div class="outline-text-2" id="text-13">
<ol class="org-ol">
<li>Is at least one of the predictors \(X_1 , X_2 , \dots , X_p\) useful in predicting the response?</li>
<li>Do all the predictors help to explain Y , or is only a subset of the predictors useful?</li>
<li>How well does the model fit the data?</li>
<li>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</li>
</ol>
</div>
</div>


<div id="outline-container-org127484c" class="outline-2">
<h2 id="org127484c"><span class="section-number-2">14</span> Is at least one of the predictors useful?</h2>
<div class="outline-text-2" id="text-14">
<p>
The approach of using an \(F\) -statistic to test for any association between
the predictors and the response works when \(p\) is relatively small, and 
certainly small compared to \(n\). However, sometimes we have a very large number
 of variables. If p &gt; n then there are more coefficients \(\beta_j\) to estimate
than observations from which to estimate them. In this case we cannot
even fit the multiple linear regression model using least squares, so the
\(F\) -statistic cannot be used, and neither can most of the other concepts that
we have seen so far
</p>
</div>
</div>


<div id="outline-container-orgdaf5e2c" class="outline-2">
<h2 id="orgdaf5e2c"><span class="section-number-2">15</span> Deciding on Important Variables</h2>
<div class="outline-text-2" id="text-15">
<p>
Ideally, we would like to perform variable selection by trying out a lot of
different models, each containing a different subset of the predictors. For
instance, if \(p = 2\), then we can consider four models: 
</p>

<ol class="org-ol">
<li>a model containing no variables,</li>
<li>a model containing \(X_1\) only,</li>
<li>a model containing \(X_2\) only,</li>
<li>a model containing both \(X_1\) and \(X_2\) .</li>
</ol>
</div>
</div>


<div id="outline-container-orga7ebf54" class="outline-2">
<h2 id="orga7ebf54"><span class="section-number-2">16</span> Deciding on Important Variables</h2>
<div class="outline-text-2" id="text-16">
<p>
We can then select the best model out of all of the models that we have considered. How
do we determine which model is best? Various statistics can be used to
judge the quality of a model. 
</p>

<p>
These include 
</p>

<ul class="org-ul">
<li>Mallow’s C<sub>p</sub> ,</li>
<li>Akaike informa-tion criterion (AIC),</li>
<li>Bayesian information criterion (BIC),</li>
<li>Adjusted \(R^2\).</li>
</ul>
</div>
</div>


<div id="outline-container-org89f0fd7" class="outline-2">
<h2 id="org89f0fd7"><span class="section-number-2">17</span> Deciding on Important Variables</h2>
<div class="outline-text-2" id="text-17">
<p>
Unless p is very
small, we cannot consider all \(2^p\) models, and instead we need an automated
and efficient approach to choose a smaller set of models to consider. There
are three classical approaches for this task:
</p>
</div>

<div id="outline-container-org17a13be" class="outline-3">
<h3 id="org17a13be"><span class="section-number-3">17.1</span> Forward selection</h3>
<div class="outline-text-3" id="text-17-1">
<p>
We begin with the null model—a model that con-
tains an intercept but no predictors. We then fit p simple linear re-
gressions and add to the null model the variable that results in the
lowest RSS. We then add to that model the variable that results
in the lowest RSS for the new two-variable model. This approach is
continued until some stopping rule is satisfied.
</p>
</div>
</div>
</div>

<div id="outline-container-org1c74709" class="outline-2">
<h2 id="org1c74709"><span class="section-number-2">18</span> </h2>
<div class="outline-text-2" id="text-18">
</div>
<div id="outline-container-org016b630" class="outline-3">
<h3 id="org016b630"><span class="section-number-3">18.1</span> Backward selection.</h3>
<div class="outline-text-3" id="text-18-1">
<p>
We start with all variables in the model, and
remove the variable with the largest p-value—that is, the variable
that is the least statistically significant. The new \((p - 1)\) -variable
model is fit, and the variable with the largest p-value is removed. This
procedure continues until a stopping rule is reached. For instance, we
may stop when all remaining variables have a p-value below some
threshold.
Backward selection cannot be used if \(p > n\).
</p>
</div>
</div>

<div id="outline-container-orgeb8c02e" class="outline-3">
<h3 id="orgeb8c02e"><span class="section-number-3">18.2</span> Mixed selection</h3>
<div class="outline-text-3" id="text-18-2">
<p>
This is a combination of forward and backward se-
lection. We start with no variables in the model, and as with forward
selection, we add the variable that provides the best fit. We con-
tinue to add variables one-by-one.
</p>

<p>
If at any point the
p-value for one of the variables in the model rises above a certain
threshold, then we remove that variable from the model. We con-
tinue to perform these forward and backward steps until all variables
in the model have a sufficiently low p-value, and all variables outside
the model would have a large p-value if added to the model.
</p>
</div>
</div>
</div>


<div id="outline-container-org4be2c22" class="outline-2">
<h2 id="org4be2c22"><span class="section-number-2">19</span> Model Fit</h2>
<div class="outline-text-2" id="text-19">
<p>
Two of the most common numerical measures of model fit are the RSE and
\(R^2\) , the fraction of variance explained. These quantities are computed and
interpreted in the same fashion as for simple linear regression.
</p>
</div>
</div>

<div id="outline-container-org602097a" class="outline-2">
<h2 id="org602097a"><span class="section-number-2">20</span> Predictions</h2>
<div class="outline-text-2" id="text-20">
<p>
There are three sorts of uncertainty associated with the Linear Model 
</p>

<ul class="org-ul">
<li>We can compute a confidence interval in order to determine how close \(\hat{Y}\) will be to \(f(X)\).</li>
<li>Model bias: we use a linear model, we are in fact estimating the best linear approximation to the true surface. However, here we will ignore this discrepancy, and operate as if the linear model were correct.</li>
<li>We use prediction intervals to answer how much will \(Y\) vary from
\(\hat{Y}\) ponitwise. Prediction intervals are always wider than
confidence intervals, because they incorporate both the error in the
estimate for f (X) (the reducible error) and the uncertainty as to
how much an individual point will differ from the population
regression plane</li>
</ul>


<p>
I found this nice video.
</p>

<p>
<a href="https://www.youtube.com/watch?v=qVCQi0KPR0s">https://www.youtube.com/watch?v=qVCQi0KPR0s</a>
</p>
</div>
</div>


<div id="outline-container-orgd40a3f2" class="outline-2">
<h2 id="orgd40a3f2"><span class="section-number-2">21</span> Example</h2>
<div class="outline-text-2" id="text-21">
<pre class="example">
fit &lt;- lm(Petal.Width ~ Petal.Length, data=iris)

plot(Petal.Width ~ Petal.Length, col=c("black", "red", "blue")[Species], 
pch=(15:17)[Species], 
xlab="Petal Length (cm)", ylab="Petal Width (cm)", data=iris)
newx &lt;- data.frame(Petal.Length=seq(min(iris$Petal.Length), max(iris$Petal.Length), length.out=100))
conf.interval &lt;- predict(fit, newdata=newx, interval="confidence")
pred.interval &lt;- predict(fit, newdata=newx, interval="prediction")
lines(conf.interval[, "fit"] ~ newx[, 1], lty=1, lw=3)
lines(conf.interval[, "lwr"] ~ newx[, 1], lty=2)
lines(conf.interval[, "upr"] ~ newx[, 1], lty=2)
lines(pred.interval[, "lwr"] ~ newx[, 1], lty=3)
lines(pred.interval[, "upr"] ~ newx[, 1], lty=3)
legend("topleft", legend=c(levels(iris$Species), "CI", "PI"), 
col=c("black", "red", "blue", "black", "black"), 
pch=c(15:17, -1, -1), lty=c(-1, -1, -1, 2, 3))
</pre>


<p>
from <a href="https://rpubs.com/lwaldron/iris_regression">https://rpubs.com/lwaldron/iris_regression</a>
</p>
</div>
</div>

<div id="outline-container-org23c8c99" class="outline-2">
<h2 id="org23c8c99"><span class="section-number-2">22</span> References</h2>
<div class="outline-text-2" id="text-22">
<p>
[1] Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani <i>An Introduction to Statistical Learning</i>. Springer Texts in Statistics, 2014.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Daniel Ballesteros-Chávez</p>
<p class="date">Created: 2022-11-29 Tue 09:39</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
